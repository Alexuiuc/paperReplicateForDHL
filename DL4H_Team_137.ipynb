{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# Before you use this template\n",
    "\n",
    "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
    "\n",
    "---\n",
    "\n",
    "# FAQ and Attentions\n",
    "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
    "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
    "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
    "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
    "must be within 8 min, otherwise, you may get penalty on the grade.\n",
    "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
    "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
    "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
    "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
    "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
    "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Github:https://github.com/Alexuiuc/paperReplicateForDHL\n",
    "2. Google colab:  https://drive.google.com/drive/folders/15PGQQOBm9GayTnmfjzrIQE3VBRx3hdC8?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfk8Zrul_E8V"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jd4m-TWyOwB"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "print(sys.version)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "*   Background of the problem\n",
    "    * This study is about readmission/mortality prediction.\n",
    "    * Unstructured data, especially for claim data which doesn't have clear structures, making models like MiME(Choi et al. 2018) not able to use.\n",
    "    * The difficulties are finding the hidden structure of the data and making the prediction at the same time.\n",
    "\n",
    "    * The paper's approach is effetive by their test metric.\n",
    "*   Paper explanation\n",
    "    * The paper proposed a new approach, Graph Convolutional Transformer (GCT),to jointly lean the hidden strucuter and do the prediction task. This method will use the unstructured data as initial input and achieve good prediction of general medial tasks.\n",
    "\n",
    "    * THE TEST METRIC FROM THE PAPER SHOWS BELOW\n",
    "    * It is a great compliment for people who cannot access the structred data. Also the learned structure can be helpful for people who want reuse the learned sturcture for their furture study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABD4VhFZbehA"
   },
   "outputs": [],
   "source": [
    "# code comment is used as inline annotations for your coding\n",
    "\n",
    "img_dir = '/content/drive/My Drive/Colab_Notebooks/original_metircs.png'\n",
    "current_dir = '/content/drive/My Drive/Colab_Notebooks'\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "img = cv2.imread(img_dir)\n",
    "\n",
    "cv2_imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
    "\n",
    "\n",
    "1.   Hypothesis 1: The github repo(https://github.com/Google-Health/records-research/tree/master/graph-convolutional-transformer) as the paper shows can run and gives a meaningful result\n",
    "2.   Hypothesis 2: GCT outperforms baseline models in task readmission prediction for a publicly available EHR dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
    "\n",
    "The methodology at least contains two subsections **data** and **model** in your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HgvyTM6NHfn"
   },
   "source": [
    "\n",
    "# Hypothesis 1: The github repo is runnable\n",
    "With some fix, the repo is runnable based on the README.md.  \n",
    "Here are the steps I run the repo code:\n",
    "\n",
    "1. fetch the repo into your local environment.  \n",
    "\n",
    "2. go to the directory of `graph-convolutional-transformer/`   \n",
    "\n",
    "3. Because of the compitable issue, remove the `import sklearn ...` in the two py files and change the function calls such as `ms.train_test_split` to `train_test_split`. Then add the function at the top of the two py files  \n",
    "\n",
    "    ```\n",
    "    import random\n",
    "    def train_test_split(data, test_size=0.2, random_state=None):\n",
    "        if random_state is not None:\n",
    "            random.seed(random_state)\n",
    "        \n",
    "        data_shuffled = data[:]\n",
    "        random.shuffle(data_shuffled)\n",
    "        \n",
    "        split_idx = int(len(data) * (1 - test_size))\n",
    "        \n",
    "        train = data_shuffled[:split_idx]\n",
    "        test = data_shuffled[split_idx:]\n",
    "        \n",
    "        return train, test\n",
    "    ```\n",
    "\n",
    "\n",
    "4. install conda and run below in terminal\n",
    "\n",
    "    ```\n",
    "    conda create -n DHLFinalProject python=2.7\n",
    "\n",
    "    conda activate DHLFinalProject\n",
    "\n",
    "    conda install tensorflow==1.13.1\n",
    "\n",
    "    mkdir output\n",
    "\n",
    "    ```\n",
    "5. Copy data downloaded from citi to the directroy.  \n",
    "\n",
    "\n",
    "6. run below in terminal to process the eicu data.    \n",
    "\n",
    "    `python eicu_samples/process_eicu.py . ./output`  \n",
    "\n",
    "\n",
    "7. remove the `.` in the `train.py` `./train.tfrecord`, `./validation.tfrecord`  \n",
    "\n",
    "8. create `trainResult/fold_0/` in the directory.  \n",
    "\n",
    "9. setting `num_iter = 4800`in `train.py`, then in terminal run: `python train.py output/fold_0 trainResult/fold_0`.  \n",
    "\n",
    "10. run for about 2hrs. We get the result in the termial as:  \n",
    "`\n",
    "INFO:tensorflow:Evaluation [10/100]\n",
    "INFO:tensorflow:Evaluation [20/100]\n",
    "INFO:tensorflow:Evaluation [30/100]\n",
    "INFO:tensorflow:Evaluation [40/100]\n",
    "INFO:tensorflow:Evaluation [50/100]\n",
    "INFO:tensorflow:Evaluation [60/100]\n",
    "INFO:tensorflow:Evaluation [70/100]\n",
    "INFO:tensorflow:Evaluation [80/100]\n",
    "INFO:tensorflow:Evaluation [90/100]\n",
    "INFO:tensorflow:Evaluation [100/100]\n",
    "INFO:tensorflow:Finished evaluation at 2024-04-13-09:18:05\n",
    "INFO:tensorflow:Saving dict for global step 4800: AUC-PR = 0.32076856, AUC-ROC = 0.6733924, global_step = 4800, loss = 0.5876697\n",
    "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4800: trainResult/fold_0/model.ckpt-4800\n",
    "INFO:tensorflow:Loss for final step: 0.31709772.\n",
    "INFO:tensorflow:Calling model_fn.\n",
    "INFO:tensorflow:Done calling model_fn.\n",
    "INFO:tensorflow:Starting evaluation at 2024-04-13T09:18:06Z\n",
    "INFO:tensorflow:Graph was finalized.\n",
    "INFO:tensorflow:Restoring parameters from trainResult/fold_0/model.ckpt-4800\n",
    "INFO:tensorflow:Running local_init_op.\n",
    "INFO:tensorflow:Done running local_init_op.\n",
    "2024-04-13 02:18:07.447873: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice_28. Error: ValidateStridedSliceOp returned partial shapes [1,?,1,101,101] and [?,1,101,101]\n",
    "2024-04-13 02:18:07.447943: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice_31. Error: ValidateStridedSliceOp returned partial shapes [1,?,1,101,101] and [?,1,101,101]\n",
    "2024-04-13 02:18:07.447990: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice_32. Error: ValidateStridedSliceOp returned partial shapes [1,?,1,101,101] and [?,1,101,101]\n",
    "INFO:tensorflow:Finished evaluation at 2024-04-13-09:19:03\n",
    "INFO:tensorflow:Saving dict for global step 4800: AUC-PR = 0.31366622, AUC-ROC = 0.676465, global_step = 4800, loss = 0.5836665\n",
    "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4800: trainResult/fold_0/model.ckpt-4800`\n",
    "\n",
    "# Note:\n",
    "1. The demo train reuslt in one fold is `AUC-PR = 0.31366622, AUC-ROC = 0.676465, global_step = 4800, loss = 0.5836665`.\n",
    "\n",
    "2. The Hypothesis 1 can only be checked in local environment because the colab doesn't support the python2.7.\n",
    "\n",
    "3. In my mac, 100 steps needs about 150 seconds to run in the train step. The model initially set the num_iter=1000000. This needs 35 days to run a five folds.\n",
    "\n",
    "4. The **data** and **model** section is included above in a local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi3oFF6fOrNi"
   },
   "source": [
    "# Hypothesis 2: GCT outperforms baseline models in task readmission prediction for a publicly available EHR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data\n",
    "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
    "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
    "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
    "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
    "  * Illustration: printing results, plotting figures for illustration.\n",
    "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-4lzfTvhvtn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "1. The eICU data comes from  `https://physionet.org/content/eicu-crd/2.0/`. You are required to participate in the CITI training.\n",
    "2. Download the patient, admissionDx, diagnosis, treatment CSV files\n",
    "3. Decompress and upload to the Drive side by the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaRhQMf1qx1s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leXGyads4kmx"
   },
   "source": [
    "# Notes\n",
    " 1. Because of the compitible isssues, it may be more approprate to rewrite the data process functions. But it takes more time than I thought. Currently It works but still have bugs.\n",
    " 2. Instead, I just fixed compitible isssues in the provided code and run the data process instead without using the pandas. I will leave pandas in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "# dir and function to load raw data\n",
    "import pandas as pd\n",
    "\n",
    "raw_data_dir = '/content/drive/My Drive/Colab_Notebooks/'\n",
    "filenames = ['patient.csv', 'admissionDx.csv', 'diagnosis.csv', 'treatment.csv']\n",
    "SUBSET_RATIO=0.1\n",
    "\n",
    "def load_raw_data(raw_data_dir, filenames, subset_ratio=1.0):\n",
    "\n",
    "    data_frames = []\n",
    "    for filename in filenames:\n",
    "        file_path = raw_data_dir + filename\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if subset_ratio < 1.0:\n",
    "            df = df.sample(frac=subset_ratio)\n",
    "        data_frames.append(df)\n",
    "\n",
    "    return data_frames\n",
    "\n",
    "raw_data_df_list = load_raw_data(raw_data_dir,filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQYpz1svw9QR"
   },
   "outputs": [],
   "source": [
    "print('patient.csv')\n",
    "raw_data_df_list[0].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DJMIztnyEYn"
   },
   "outputs": [],
   "source": [
    "print('admissionDx.csv')\n",
    "raw_data_df_list[1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Mvs0iXKxo0-"
   },
   "outputs": [],
   "source": [
    "print('diagnosis.csv')\n",
    "raw_data_df_list[2].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhRUITxqxyDt"
   },
   "outputs": [],
   "source": [
    "print('treatment.csv')\n",
    "raw_data_df_list[3].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNsUtkhsx7Ox"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # process raw data\n",
    "# def process_data(raw_data):\n",
    "#     # implement this function to process the data as you need\n",
    "#   return None\n",
    "\n",
    "# processed_data = process_data(raw_data_df_list)\n",
    "\n",
    "# ''' you can load the processed data directly\n",
    "# processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
    "# def load_processed_data(raw_data_dir):\n",
    "#   pass\n",
    "\n",
    "# '''\n",
    "\n",
    "# calculate statistics\n",
    "# include basic descriptive statistics of the dataset like size, cross validation split, label distribution\n",
    "def calculate_stats(raw_data,filenames):\n",
    "\n",
    "    stats = [data.describe() for data in  raw_data]\n",
    "    for filename,stat in zip(filenames,stats):\n",
    "        print(\"Basic statistics for {}:\\n {}\".format(filename,stat))\n",
    "        print('\\n')\n",
    "\n",
    "calculate_stats(raw_data_df_list, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG5YU3rulnS0"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# class EncounterInfo(object):\n",
    "#     def __init__(self, patient_id, encounter_id, encounter_timestamp, expired, readmission):\n",
    "#         self.patient_id = patient_id\n",
    "#         self.encounter_id = encounter_id\n",
    "#         self.encounter_timestamp = encounter_timestamp\n",
    "#         self.expired = expired\n",
    "#         self.readmission = readmission\n",
    "#         self.dx_ids = []\n",
    "#         self.rx_ids = []\n",
    "#         self.labs = {}\n",
    "#         self.physicals = []\n",
    "#         self.treatments = []\n",
    "#     def __repr__(self):\n",
    "#         return (f\"EncounterInfo(\\n\"\n",
    "#                 f\"  Patient ID: {self.patient_id},\\n\"\n",
    "#                 f\"  Encounter ID: {self.encounter_id},\\n\"\n",
    "#                 f\"  Timestamp: {self.encounter_timestamp},\\n\"\n",
    "#                 f\"  Expired: {'Yes' if self.expired else 'No'},\\n\"\n",
    "#                 f\"  Readmission: {'Yes' if self.readmission else 'No'},\\n\"\n",
    "#                 f\"  Diagnosis IDs: {self.dx_ids},\\n\"\n",
    "#                 f\"  Prescription IDs: {self.rx_ids},\\n\"\n",
    "#                 f\"  Labs: {self.labs},\\n\"\n",
    "#                 f\"  Physical Exams: {self.physicals},\\n\"\n",
    "#                 f\"  Treatments: {self.treatments}\\n)\")\n",
    "\n",
    "\n",
    "# def process_patient(df, hour_threshold=24):\n",
    "#     # Calculate encounter_timestamp and create a temporary DataFrame for sorting\n",
    "#     df['encounter_timestamp'] = -df['hospitaladmitoffset'].astype(int)\n",
    "\n",
    "#     # Sorting patients by their IDs and then by the encounter timestamp\n",
    "#     df_sorted = df.sort_values(['patienthealthsystemstayid', 'encounter_timestamp'])\n",
    "\n",
    "#     # Detect readmissions by checking if the next stay is within the same patient ID\n",
    "#     df_sorted['next_patient_id'] = df_sorted['patienthealthsystemstayid'].shift(-1)\n",
    "#     df_sorted['readmission'] = df_sorted['patienthealthsystemstayid'] == df_sorted['next_patient_id']\n",
    "\n",
    "#     # Mark the last encounter for each patient as not a readmission\n",
    "#     df_sorted.loc[df_sorted['patienthealthsystemstayid'] != df_sorted['next_patient_id'], 'readmission'] = False\n",
    "\n",
    "#     # Process each encounter\n",
    "#     encounter_dict = {}\n",
    "#     for _, row in df_sorted.iterrows():\n",
    "#         duration_minute = float(row['unitdischargeoffset'])\n",
    "\n",
    "#         if duration_minute > 60. * hour_threshold:\n",
    "#             continue\n",
    "\n",
    "#         expired = True if row['unitdischargestatus'] == 'Expired' else False\n",
    "#         encounter_id = row['patientunitstayid']\n",
    "\n",
    "#         # Instantiate EncounterInfo\n",
    "#         ei = EncounterInfo(\n",
    "#             patient_id=row['patienthealthsystemstayid'],\n",
    "#             encounter_id=encounter_id,\n",
    "#             encounter_timestamp=row['encounter_timestamp'],\n",
    "#             expired=expired,\n",
    "#             readmission=row['readmission']\n",
    "#         )\n",
    "\n",
    "#         if encounter_id in encounter_dict:\n",
    "#             raise ValueError('Duplicate encounter ID!!')\n",
    "\n",
    "#         encounter_dict[encounter_id] = ei\n",
    "\n",
    "#     return encounter_dict\n",
    "\n",
    "# encounter_dict = {}\n",
    "# encounter_dict = process_patient(raw_data_df_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1-eYZ-Lk9bz"
   },
   "outputs": [],
   "source": [
    "# print(next(iter(encounter_dict.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UoDoLpcnZP1"
   },
   "outputs": [],
   "source": [
    "# def process_admission_dx(df, encounter_dict):\n",
    "#     # Check and report the number of missing encounter IDs\n",
    "#     missing_eid = df[~df['patientunitstayid'].isin(encounter_dict.keys())]\n",
    "#     print('Admission Diagnosis without Encounter ID:', len(missing_eid))\n",
    "\n",
    "#     # Filter out rows where the encounter_id is not in encounter_dict\n",
    "#     df = df[df['patientunitstayid'].isin(encounter_dict.keys())]\n",
    "\n",
    "#     # Process each row in the DataFrame\n",
    "#     for idx, row in df.iterrows():\n",
    "#         encounter_id = row['patientunitstayid']\n",
    "#         dx_id = row['admitdxpath'].lower()\n",
    "\n",
    "#         if encounter_id in encounter_dict:\n",
    "#             encounter_dict[encounter_id].dx_ids.append(dx_id)\n",
    "\n",
    "#     return encounter_dict\n",
    "# encounter_dict = process_admission_dx(raw_data_df_list[1],encounter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL72VXsOnp2-"
   },
   "outputs": [],
   "source": [
    "# print(next(iter(encounter_dict.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2lgaZ5ipVGc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DRt4V7Oor4S"
   },
   "outputs": [],
   "source": [
    "# def process_diagnosis(df, encounter_dict):\n",
    "#     # Count and report the number of missing encounter IDs\n",
    "#     missing_eid = df[~df['patientunitstayid'].isin(encounter_dict.keys())]\n",
    "#     print('Diagnosis without Encounter ID:', len(missing_eid))\n",
    "\n",
    "#     # Filter the DataFrame to include only rows with encounter IDs that exist in the encounter_dict\n",
    "#     df = df[df['patientunitstayid'].isin(encounter_dict.keys())]\n",
    "\n",
    "#     # Process each row to append diagnosis strings to the appropriate EncounterInfo object\n",
    "#     for _, row in df.iterrows():\n",
    "#         encounter_id = row['patientunitstayid']\n",
    "#         dx_id = row['diagnosisstring'].lower()  # Assuming case insensitivity is desired\n",
    "\n",
    "#         if encounter_id in encounter_dict:\n",
    "#             encounter_dict[encounter_id].dx_ids.append(dx_id)\n",
    "\n",
    "#     return encounter_dict\n",
    "# encounter_dict = process_diagnosis(raw_data_df_list[2],encounter_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsSknv0SpXiA"
   },
   "outputs": [],
   "source": [
    "# dict_iter = iter(encounter_dict.items())\n",
    "# print(next(dict_iter))\n",
    "# print(next(dict_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOm35bYjpf3K"
   },
   "outputs": [],
   "source": [
    "# def process_treatment(df, encounter_dict):\n",
    "#     # Identify and count missing encounter IDs that are not in the encounter_dict\n",
    "#     missing_eid = df[~df['patientunitstayid'].isin(encounter_dict.keys())]\n",
    "#     print('Treatment without Encounter ID:', len(missing_eid))\n",
    "\n",
    "#     # Filter the DataFrame to include only rows with encounter IDs that exist in the encounter_dict\n",
    "#     df_filtered = df[df['patientunitstayid'].isin(encounter_dict.keys())]\n",
    "\n",
    "#     # Process each row to append treatment strings to the appropriate EncounterInfo object\n",
    "#     count = 0\n",
    "#     for _, row in df_filtered.iterrows():\n",
    "#         encounter_id = row['patientunitstayid']\n",
    "#         treatment_id = row['treatmentstring'].lower()  # Assuming case insensitivity is desired\n",
    "\n",
    "#         if encounter_id in encounter_dict:\n",
    "#             encounter_dict[encounter_id].treatments.append(treatment_id)\n",
    "#             count += 1\n",
    "\n",
    "#     print('Accepted treatments:', count)\n",
    "#     return encounter_dict\n",
    "\n",
    "# encounter_dict = process_diagnosis(raw_data_df_list[3],encounter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b74kRBvxpmZx"
   },
   "outputs": [],
   "source": [
    "# dict_iter = iter(encounter_dict.items())\n",
    "# print(next(dict_iter))\n",
    "# print(next(dict_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvAiWT-p6B3I"
   },
   "outputs": [],
   "source": [
    "# fix compitible issue of the github https://github.com/Alexuiuc/records-research/tree/master/graph-convolutional-transformer:\n",
    "\n",
    "\"\"\"Copyright 2019 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "def train_test_split(data, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "\n",
    "    data_shuffled = data[:]\n",
    "    random.shuffle(data_shuffled)\n",
    "\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "\n",
    "    train = data_shuffled[:split_idx]\n",
    "    test = data_shuffled[split_idx:]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "class EncounterInfo(object):\n",
    "\n",
    "  def __init__(self, patient_id, encounter_id, encounter_timestamp, expired,\n",
    "               readmission):\n",
    "    self.patient_id = patient_id\n",
    "    self.encounter_id = encounter_id\n",
    "    self.encounter_timestamp = encounter_timestamp\n",
    "    self.expired = expired\n",
    "    self.readmission = readmission\n",
    "    self.dx_ids = []\n",
    "    self.rx_ids = []\n",
    "    self.labs = {}\n",
    "    self.physicals = []\n",
    "    self.treatments = []\n",
    "\n",
    "\n",
    "def process_patient(infile, encounter_dict, hour_threshold=24):\n",
    "\n",
    "  inff = open(infile, 'r')\n",
    "\n",
    "  count = 0\n",
    "  patient_dict = {}\n",
    "  for line in csv.DictReader(inff):\n",
    "\n",
    "    patient_id = line['patienthealthsystemstayid']\n",
    "    encounter_id = line['patientunitstayid']\n",
    "    encounter_timestamp = -int(line['hospitaladmitoffset'])\n",
    "    if patient_id not in patient_dict:\n",
    "      patient_dict[patient_id] = []\n",
    "    patient_dict[patient_id].append((encounter_timestamp, encounter_id))\n",
    "  inff.close()\n",
    "\n",
    "\n",
    "  patient_dict_sorted = {}\n",
    "  for patient_id, time_enc_tuples in patient_dict.items():\n",
    "    patient_dict_sorted[patient_id] = sorted(time_enc_tuples)\n",
    "\n",
    "  enc_readmission_dict = {}\n",
    "  for patient_id, time_enc_tuples in patient_dict_sorted.items():\n",
    "    for time_enc_tuple in time_enc_tuples[:-1]:\n",
    "      enc_id = time_enc_tuple[1]\n",
    "      enc_readmission_dict[enc_id] = True\n",
    "    last_enc_id = time_enc_tuples[-1][1]\n",
    "    enc_readmission_dict[last_enc_id] = False\n",
    "\n",
    "  inff = open(infile, 'r')\n",
    "  count = 0\n",
    "\n",
    "  for line in csv.DictReader(inff):\n",
    "\n",
    "    patient_id = line['patienthealthsystemstayid']\n",
    "    encounter_id = line['patientunitstayid']\n",
    "    encounter_timestamp = -int(line['hospitaladmitoffset'])\n",
    "    discharge_status = line['unitdischargestatus']\n",
    "    duration_minute = float(line['unitdischargeoffset'])\n",
    "    expired = True if discharge_status == 'Expired' else False\n",
    "    readmission = enc_readmission_dict[encounter_id]\n",
    "\n",
    "    if duration_minute > 60. * hour_threshold:\n",
    "      continue\n",
    "\n",
    "    ei = EncounterInfo(patient_id, encounter_id, encounter_timestamp, expired,\n",
    "                       readmission)\n",
    "    if encounter_id in encounter_dict:\n",
    "      print('Duplicate encounter ID!!')\n",
    "      sys.exit(0)\n",
    "    encounter_dict[encounter_id] = ei\n",
    "    count += 1\n",
    "\n",
    "  inff.close()\n",
    "\n",
    "\n",
    "\n",
    "  return encounter_dict\n",
    "\n",
    "\n",
    "def process_admission_dx(infile, encounter_dict):\n",
    "  inff = open(infile, 'r')\n",
    "  count = 0\n",
    "  missing_eid = 0\n",
    "  for line in csv.DictReader(inff):\n",
    "\n",
    "    encounter_id = line['patientunitstayid']\n",
    "    dx_id = line['admitdxpath'].lower()\n",
    "\n",
    "    if encounter_id not in encounter_dict:\n",
    "      missing_eid += 1\n",
    "      continue\n",
    "    encounter_dict[encounter_id].dx_ids.append(dx_id)\n",
    "    count += 1\n",
    "  inff.close()\n",
    "  print('')\n",
    "  print('Admission Diagnosis without Encounter ID: %d' % missing_eid)\n",
    "\n",
    "  return encounter_dict\n",
    "\n",
    "\n",
    "def process_diagnosis(infile, encounter_dict):\n",
    "  inff = open(infile, 'r')\n",
    "  count = 0\n",
    "  missing_eid = 0\n",
    "  for line in csv.DictReader(inff):\n",
    "\n",
    "    encounter_id = line['patientunitstayid']\n",
    "    dx_id = line['diagnosisstring'].lower()\n",
    "\n",
    "    if encounter_id not in encounter_dict:\n",
    "      missing_eid += 1\n",
    "      continue\n",
    "    encounter_dict[encounter_id].dx_ids.append(dx_id)\n",
    "    count += 1\n",
    "  inff.close()\n",
    "  print('')\n",
    "  print('Diagnosis without Encounter ID: %d' % missing_eid)\n",
    "\n",
    "  return encounter_dict\n",
    "\n",
    "\n",
    "def process_treatment(infile, encounter_dict):\n",
    "  inff = open(infile, 'r')\n",
    "  count = 0\n",
    "  missing_eid = 0\n",
    "\n",
    "  for line in csv.DictReader(inff):\n",
    "\n",
    "    encounter_id = line['patientunitstayid']\n",
    "    treatment_id = line['treatmentstring'].lower()\n",
    "\n",
    "    if encounter_id not in encounter_dict:\n",
    "      missing_eid += 1\n",
    "      continue\n",
    "    encounter_dict[encounter_id].treatments.append(treatment_id)\n",
    "    count += 1\n",
    "  inff.close()\n",
    "  print('')\n",
    "  print('Treatment without Encounter ID: %d' % missing_eid)\n",
    "  print('Accepted treatments: %d' % count)\n",
    "\n",
    "  return encounter_dict\n",
    "\n",
    "\n",
    "def build_seqex(enc_dict,\n",
    "                skip_duplicate=False,\n",
    "                min_num_codes=1,\n",
    "                max_num_codes=50):\n",
    "  key_list = []\n",
    "  seqex_list = []\n",
    "  dx_str2int = {}\n",
    "  treat_str2int = {}\n",
    "  num_cut = 0\n",
    "  num_duplicate = 0\n",
    "  count = 0\n",
    "  num_dx_ids = 0\n",
    "  num_treatments = 0\n",
    "  num_unique_dx_ids = 0\n",
    "  num_unique_treatments = 0\n",
    "  min_dx_cut = 0\n",
    "  min_treatment_cut = 0\n",
    "  max_dx_cut = 0\n",
    "  max_treatment_cut = 0\n",
    "  num_expired = 0\n",
    "  num_readmission = 0\n",
    "\n",
    "  for _, enc in enc_dict.items():\n",
    "    if skip_duplicate:\n",
    "      if (len(enc.dx_ids) > len(set(enc.dx_ids)) or\n",
    "          len(enc.treatments) > len(set(enc.treatments))):\n",
    "        num_duplicate += 1\n",
    "        continue\n",
    "\n",
    "    if len(set(enc.dx_ids)) < min_num_codes:\n",
    "      min_dx_cut += 1\n",
    "      continue\n",
    "\n",
    "    if len(set(enc.treatments)) < min_num_codes:\n",
    "      min_treatment_cut += 1\n",
    "      continue\n",
    "\n",
    "    if len(set(enc.dx_ids)) > max_num_codes:\n",
    "      max_dx_cut += 1\n",
    "      continue\n",
    "\n",
    "    if len(set(enc.treatments)) > max_num_codes:\n",
    "      max_treatment_cut += 1\n",
    "      continue\n",
    "\n",
    "    count += 1\n",
    "    num_dx_ids += len(enc.dx_ids)\n",
    "    num_treatments += len(enc.treatments)\n",
    "    num_unique_dx_ids += len(set(enc.dx_ids))\n",
    "    num_unique_treatments += len(set(enc.treatments))\n",
    "\n",
    "    for dx_id in enc.dx_ids:\n",
    "      if dx_id not in dx_str2int:\n",
    "        dx_str2int[dx_id] = len(dx_str2int)\n",
    "\n",
    "    for treat_id in enc.treatments:\n",
    "      if treat_id not in treat_str2int:\n",
    "        treat_str2int[treat_id] = len(treat_str2int)\n",
    "\n",
    "    seqex = tf.train.SequenceExample()\n",
    "    seqex.context.feature['patientId'].bytes_list.value.append((enc.patient_id +\n",
    "                                                               ':' +\n",
    "                                                               enc.encounter_id).encode('utf-8'))\n",
    "    if enc.expired:\n",
    "      seqex.context.feature['label.expired'].int64_list.value.append(1)\n",
    "      num_expired += 1\n",
    "    else:\n",
    "      seqex.context.feature['label.expired'].int64_list.value.append(0)\n",
    "\n",
    "    if enc.readmission:\n",
    "      seqex.context.feature['label.readmission'].int64_list.value.append(1)\n",
    "      num_readmission += 1\n",
    "    else:\n",
    "      seqex.context.feature['label.readmission'].int64_list.value.append(0)\n",
    "\n",
    "    dx_ids = seqex.feature_lists.feature_list['dx_ids']\n",
    "\n",
    "    dx_ids_bytes = [dx_id.encode('utf-8') for dx_id in set(enc.dx_ids)]\n",
    "\n",
    "    dx_ids.feature.add().bytes_list.value.extend(dx_ids_bytes)\n",
    "\n",
    "    dx_int_list = [dx_str2int[item] for item in set(enc.dx_ids)]\n",
    "    dx_ints = seqex.feature_lists.feature_list['dx_ints']\n",
    "    dx_ints.feature.add().int64_list.value.extend(dx_int_list)\n",
    "\n",
    "\n",
    "    proc_ids = seqex.feature_lists.feature_list['proc_ids']\n",
    "    proc_ids_bytes = [item.encode('utf-8') for item in set(enc.treatments)]\n",
    "    proc_ids.feature.add().bytes_list.value.extend(proc_ids_bytes)\n",
    "\n",
    "\n",
    "    proc_int_list = [treat_str2int[item] for item in set(enc.treatments)]\n",
    "    proc_ints = seqex.feature_lists.feature_list['proc_ints']\n",
    "    proc_ints.feature.add().int64_list.value.extend(proc_int_list)\n",
    "\n",
    "\n",
    "    seqex_list.append(seqex)\n",
    "    key = seqex.context.feature['patientId'].bytes_list.value[0]  # assuming this is set somewhere as bytes\n",
    "    key_list.append(key)\n",
    "\n",
    "  print('Filtered encounters due to duplicate codes: %d' % num_duplicate)\n",
    "  print('Filtered encounters due to thresholding: %d' % num_cut)\n",
    "  print('Average num_dx_ids: %f' % (num_dx_ids / count))\n",
    "  print('Average num_treatments: %f' % (num_treatments / count))\n",
    "  print('Average num_unique_dx_ids: %f' % (num_unique_dx_ids / count))\n",
    "  print('Average num_unique_treatments: %f' % (num_unique_treatments / count))\n",
    "  print('Min dx cut: %d' % min_dx_cut)\n",
    "  print('Min treatment cut: %d' % min_treatment_cut)\n",
    "  print('Max dx cut: %d' % max_dx_cut)\n",
    "  print('Max treatment cut: %d' % max_treatment_cut)\n",
    "  print('Number of expired: %d' % num_expired)\n",
    "  print('Number of readmission: %d' % num_readmission)\n",
    "\n",
    "  return key_list, seqex_list, dx_str2int, treat_str2int\n",
    "\n",
    "\n",
    "def select_train_valid_test(key_list, random_seed=1234):\n",
    "  key_train, key_temp = train_test_split(\n",
    "      key_list, test_size=0.2, random_state=random_seed)\n",
    "  key_valid, key_test = train_test_split(\n",
    "      key_temp, test_size=0.5, random_state=random_seed)\n",
    "  return key_train, key_valid, key_test\n",
    "\n",
    "\n",
    "def count_conditional_prob_dp(seqex_list, output_path, train_key_set=None):\n",
    "  dx_freqs = {}\n",
    "  proc_freqs = {}\n",
    "  dp_freqs = {}\n",
    "  total_visit = 0\n",
    "  for seqex in seqex_list:\n",
    "    if total_visit % 1000 == 0:\n",
    "      sys.stdout.write('Visit count: %d\\r' % total_visit)\n",
    "      sys.stdout.flush()\n",
    "\n",
    "    key = seqex.context.feature['patientId'].bytes_list.value[0]\n",
    "    if (train_key_set is not None and key not in train_key_set):\n",
    "      total_visit += 1\n",
    "      continue\n",
    "\n",
    "    dx_ids = seqex.feature_lists.feature_list['dx_ids'].feature[\n",
    "        0].bytes_list.value\n",
    "    proc_ids = seqex.feature_lists.feature_list['proc_ids'].feature[\n",
    "        0].bytes_list.value\n",
    "\n",
    "    for dx in dx_ids:\n",
    "      if dx not in dx_freqs:\n",
    "        dx_freqs[dx] = 0\n",
    "      dx_freqs[dx] += 1\n",
    "\n",
    "    for proc in proc_ids:\n",
    "      if proc not in proc_freqs:\n",
    "        proc_freqs[proc] = 0\n",
    "      proc_freqs[proc] += 1\n",
    "\n",
    "    for dx in dx_ids:\n",
    "      for proc in proc_ids:\n",
    "        dp = dx + b',' + proc\n",
    "        if dp not in dp_freqs:\n",
    "          dp_freqs[dp] = 0\n",
    "        dp_freqs[dp] += 1\n",
    "\n",
    "    total_visit += 1\n",
    "\n",
    "  dx_probs = dict([(k, v / float(total_visit)) for k, v in dx_freqs.items()\n",
    "                  ])\n",
    "  proc_probs = dict([\n",
    "      (k, v / float(total_visit)) for k, v in proc_freqs.items()\n",
    "  ])\n",
    "  dp_probs = dict([(k, v / float(total_visit)) for k, v in dp_freqs.items()\n",
    "                  ])\n",
    "\n",
    "  dp_cond_probs = {}\n",
    "  pd_cond_probs = {}\n",
    "  for dx, dx_prob in dx_probs.items():\n",
    "    for proc, proc_prob in proc_probs.items():\n",
    "      dp = dx + b',' + proc\n",
    "      pd = proc + b',' + dx\n",
    "      if dp in dp_probs:\n",
    "        dp_cond_probs[dp] = dp_probs[dp] / dx_prob\n",
    "        pd_cond_probs[pd] = dp_probs[dp] / proc_prob\n",
    "      else:\n",
    "        dp_cond_probs[dp] = 0.0\n",
    "        pd_cond_probs[pd] = 0.0\n",
    "\n",
    "  pickle.dump(dx_probs, open(output_path + '/dx_probs.empirical.p', 'wb'), -1)\n",
    "  pickle.dump(proc_probs, open(output_path + '/proc_probs.empirical.p', 'wb'),\n",
    "              -1)\n",
    "  pickle.dump(dp_probs, open(output_path + '/dp_probs.empirical.p', 'wb'), -1)\n",
    "  pickle.dump(dp_cond_probs,\n",
    "              open(output_path + '/dp_cond_probs.empirical.p', 'wb'), -1)\n",
    "  pickle.dump(pd_cond_probs,\n",
    "              open(output_path + '/pd_cond_probs.empirical.p', 'wb'), -1)\n",
    "\n",
    "\n",
    "def add_sparse_prior_guide_dp(seqex_list,\n",
    "                              stats_path,\n",
    "                              key_set=None,\n",
    "                              max_num_codes=50):\n",
    "  print('Loading conditional probabilities.')\n",
    "  dp_cond_probs = pickle.load(\n",
    "      open(stats_path + '/dp_cond_probs.empirical.p', 'rb'))\n",
    "  pd_cond_probs = pickle.load(\n",
    "      open(stats_path + '/pd_cond_probs.empirical.p', 'rb'))\n",
    "\n",
    "  print('Adding prior guide.')\n",
    "  total_visit = 0\n",
    "  new_seqex_list = []\n",
    "  for seqex in seqex_list:\n",
    "    if total_visit % 1000 == 0:\n",
    "      sys.stdout.write('Visit count: %d\\r' % total_visit)\n",
    "      sys.stdout.flush()\n",
    "\n",
    "    key = seqex.context.feature['patientId'].bytes_list.value[0]\n",
    "    if (key_set is not None and key not in key_set):\n",
    "      total_visit += 1\n",
    "      continue\n",
    "\n",
    "    dx_ids = seqex.feature_lists.feature_list['dx_ids'].feature[\n",
    "        0].bytes_list.value\n",
    "    proc_ids = seqex.feature_lists.feature_list['proc_ids'].feature[\n",
    "        0].bytes_list.value\n",
    "\n",
    "    indices = []\n",
    "    values = []\n",
    "    for i, dx in enumerate(dx_ids):\n",
    "      for j, proc in enumerate(proc_ids):\n",
    "        dp = dx + b',' + proc\n",
    "        indices.append((i, max_num_codes + j))\n",
    "        prob = 0.0 if dp not in dp_cond_probs else dp_cond_probs[dp]\n",
    "        values.append(prob)\n",
    "\n",
    "    for i, proc in enumerate(proc_ids):\n",
    "      for j, dx in enumerate(dx_ids):\n",
    "        pd = proc + b',' + dx\n",
    "        indices.append((max_num_codes + i, j))\n",
    "        prob = 0.0 if pd not in pd_cond_probs else pd_cond_probs[pd]\n",
    "        values.append(prob)\n",
    "\n",
    "    indices = list(np.array(indices).reshape([-1]))\n",
    "    indices_feature = seqex.feature_lists.feature_list['prior_indices']\n",
    "    indices_feature.feature.add().int64_list.value.extend(indices)\n",
    "    values_feature = seqex.feature_lists.feature_list['prior_values']\n",
    "    values_feature.feature.add().float_list.value.extend(values)\n",
    "\n",
    "    new_seqex_list.append(seqex)\n",
    "    total_visit += 1\n",
    "\n",
    "  return new_seqex_list\n",
    "\n",
    "\n",
    "\"\"\"Set <input_path> to where the raw eICU CSV files are located.\n",
    "Set <output_path> to where you want the output files to be.\n",
    "\"\"\"\n",
    "def data_process():\n",
    "  input_path = current_dir\n",
    "  output_path = current_dir+'/output'\n",
    "  num_fold = 3\n",
    "\n",
    "  patient_file = input_path + '/patient.csv'\n",
    "  admission_dx_file = input_path + '/admissionDx.csv'\n",
    "  diagnosis_file = input_path + '/diagnosis.csv'\n",
    "  treatment_file = input_path + '/treatment.csv'\n",
    "\n",
    "  encounter_dict = {}\n",
    "  print('Processing patient.csv')\n",
    "  encounter_dict = process_patient(\n",
    "      patient_file, encounter_dict, hour_threshold=24)\n",
    "  print('Processing admission diagnosis.csv')\n",
    "  encounter_dict = process_admission_dx(admission_dx_file, encounter_dict)\n",
    "  print('Processing diagnosis.csv')\n",
    "  encounter_dict = process_diagnosis(diagnosis_file, encounter_dict)\n",
    "  print('Processing treatment.csv')\n",
    "  encounter_dict = process_treatment(treatment_file, encounter_dict)\n",
    "\n",
    "  key_list, seqex_list, dx_map, proc_map = build_seqex(\n",
    "      encounter_dict, skip_duplicate=False, min_num_codes=1, max_num_codes=50)\n",
    "\n",
    "  pickle.dump(dx_map, open(output_path + '/dx_map.p', 'wb'), -1)\n",
    "  pickle.dump(proc_map, open(output_path + '/proc_map.p', 'wb'), -1)\n",
    "\n",
    "  for i in range(num_fold):\n",
    "    fold_path = output_path + '/fold_' + str(i)\n",
    "    stats_path = fold_path + '/train_stats'\n",
    "    os.makedirs(stats_path)\n",
    "\n",
    "    key_train, key_valid, key_test = select_train_valid_test(\n",
    "        key_list, random_seed=i)\n",
    "\n",
    "    count_conditional_prob_dp(seqex_list, stats_path, set(key_train))\n",
    "    train_seqex = add_sparse_prior_guide_dp(\n",
    "        seqex_list, stats_path, set(key_train), max_num_codes=50)\n",
    "    validation_seqex = add_sparse_prior_guide_dp(\n",
    "        seqex_list, stats_path, set(key_valid), max_num_codes=50)\n",
    "    test_seqex = add_sparse_prior_guide_dp(\n",
    "        seqex_list, stats_path, set(key_test), max_num_codes=50)\n",
    "\n",
    "    with tf.io.TFRecordWriter(fold_path + '/train.tfrecord') as writer:\n",
    "      for seqex in train_seqex:\n",
    "        writer.write(seqex.SerializeToString())\n",
    "\n",
    "    with tf.io.TFRecordWriter(fold_path + '/validation.tfrecord') as writer:\n",
    "      for seqex in validation_seqex:\n",
    "        writer.write(seqex.SerializeToString())\n",
    "\n",
    "    with tf.io.TFRecordWriter(fold_path + '/test.tfrecord') as writer:\n",
    "      for seqex in test_seqex:\n",
    "        writer.write(seqex.SerializeToString())\n",
    "\n",
    "\n",
    "%time data_process()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xb3SqMpsSUx"
   },
   "source": [
    "# Note\n",
    "1. I attempted to convert the model to a version compatible with TensorFlow 2.0 and Python 3, but encountered internal errors that made training impossible.\n",
    "2. One possible reason is that the Estimator used by TensorFlow is quite outdated, and I may have mixed some API usages.\n",
    "3. The next steps are:\n",
    "  - Try to debug line by line, because it can run on my local machine's Python 2.7.\n",
    "  - Convert the model to PyTorch. I have reviewed the model's 2,000 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class my_model():\n",
    "#   # use this class to define your model\n",
    "#   pass\n",
    "\n",
    "# model = my_model()\n",
    "# loss_func = None\n",
    "# optimizer = None\n",
    "\n",
    "# def train_model_one_iter(model, loss_func, optimizer):\n",
    "#   pass\n",
    "\n",
    "# num_epoch = 10\n",
    "# # model training loop: it is better to print the training/validation losses during the training\n",
    "# for i in range(num_epoch):\n",
    "#   train_model_one_iter(model, loss_func, optimizer)\n",
    "#   train_loss, valid_loss = None, None\n",
    "#   print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n",
    "\"\"\"Copyright 2019 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "\n",
    "class FeatureEmbedder(object):\n",
    "    def __init__(self, vocab_sizes, feature_keys, embedding_size):\n",
    "        \"\"\"Initialize the feature embedder.\"\"\"\n",
    "        self._params = {}\n",
    "        self._feature_keys = feature_keys\n",
    "        self._vocab_sizes = vocab_sizes\n",
    "        dummy_emb = tf.zeros([1, embedding_size], dtype=tf.float32)\n",
    "\n",
    "        for feature_key in feature_keys:\n",
    "            vocab_size = self._vocab_sizes[feature_key]\n",
    "            emb = tf.Variable(tf.random.uniform([vocab_size, embedding_size]), name=feature_key)\n",
    "            self._params[feature_key] = tf.concat([emb, dummy_emb], axis=0)\n",
    "\n",
    "        self._params['visit'] = tf.Variable(tf.random.uniform([1, embedding_size]), name='visit')\n",
    "\n",
    "    def lookup(self, feature_map, max_num_codes):\n",
    "        \"\"\"Convert SparseTensors to dense embeddings and masks.\"\"\"\n",
    "        masks = {}\n",
    "        embeddings = {}\n",
    "        for key in self._feature_keys:\n",
    "            if max_num_codes > 0:\n",
    "                feature = tf.SparseTensor(\n",
    "                indices=feature_map[key].indices,\n",
    "                values=feature_map[key].values,\n",
    "                dense_shape=[\n",
    "                    feature_map[key].dense_shape[0],\n",
    "                    feature_map[key].dense_shape[1], max_num_codes\n",
    "                ])\n",
    "            else:\n",
    "                feature = feature_map[key]\n",
    "            feature_ids = tf.sparse.to_dense(\n",
    "              feature, default_value=self._vocab_sizes[key])\n",
    "            feature_ids = tf.squeeze(feature_ids, axis=1)\n",
    "            embeddings[key] = tf.nn.embedding_lookup(self._params[key], feature_ids)\n",
    "\n",
    "            mask = tf.SparseTensor(\n",
    "              indices=feature.indices,\n",
    "              values=tf.ones(tf.shape(feature.values)),\n",
    "              dense_shape=feature.dense_shape)\n",
    "            masks[key] = tf.squeeze(tf.sparse.to_dense(mask), axis=1)\n",
    "\n",
    "        batch_size = tf.shape(list(embeddings.values())[0])[0]\n",
    "        embeddings['visit'] = tf.tile(self._params['visit'][None, :, :],\n",
    "                                      [batch_size, 1, 1])\n",
    "\n",
    "        masks['visit'] = tf.ones(batch_size)[:, None]\n",
    "\n",
    "        return embeddings, masks\n",
    "\n",
    "\n",
    "\n",
    "class GraphConvolutionalTransformer(tf.keras.layers.Layer):\n",
    "    \"\"\"Init function.\n",
    "\n",
    "    Args:\n",
    "      embedding_size: The size of the dimension for hidden layers.\n",
    "      num_transformer_stack: The number of Transformer blocks.\n",
    "      num_feedforward: The number of layers in the feedforward part of\n",
    "        Transformer.\n",
    "      num_attention_heads: The number of attention heads.\n",
    "      ffn_dropout: Dropout rate used inside the feedforward part.\n",
    "      attention_normalizer: Use either 'softmax' or 'sigmoid' to normalize the\n",
    "        attention values.\n",
    "      multihead_attention_aggregation: Use either 'concat' or 'sum' to handle\n",
    "        the outputs from multiple attention heads.\n",
    "      directed_attention: Decide whether you want to use the unidirectional\n",
    "        attention, where information accumulates inside the dummy visit node.\n",
    "      use_inf_mask: Decide whether you want to use the guide matrix. Currently\n",
    "        unused.\n",
    "      use_prior: Decide whether you want to use the conditional probablility\n",
    "        information. Currently unused.\n",
    "      **kwargs: Other arguments to tf.keras.layers.Layer init.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               embedding_size=128,\n",
    "               num_transformer_stack=3,\n",
    "               num_feedforward=2,\n",
    "               num_attention_heads=1,\n",
    "               ffn_dropout=0.1,\n",
    "               attention_normalizer='softmax',\n",
    "               multihead_attention_aggregation='concat',\n",
    "               directed_attention=False,\n",
    "               use_inf_mask=True,\n",
    "               use_prior=True,\n",
    "               **kwargs):\n",
    "\n",
    "        super(GraphConvolutionalTransformer, self).__init__(**kwargs)\n",
    "        self._hidden_size = embedding_size\n",
    "        self._num_stack = num_transformer_stack\n",
    "        self._num_feedforward = num_feedforward\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._ffn_dropout = ffn_dropout\n",
    "        self._attention_normalizer = attention_normalizer\n",
    "        self._multihead_aggregation = multihead_attention_aggregation\n",
    "        self._directed_attention = directed_attention\n",
    "        self._use_inf_mask = use_inf_mask\n",
    "        self._use_prior = use_prior\n",
    "\n",
    "        self._layers = {}\n",
    "        self._layers['Q'] = []\n",
    "        self._layers['K'] = []\n",
    "        self._layers['V'] = []\n",
    "        self._layers['ffn'] = []\n",
    "        self._layers['head_agg'] = []\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(axis=2)\n",
    "\n",
    "        for i in range(self._num_stack):\n",
    "            self._layers['Q'].append(\n",
    "              tf.keras.layers.Dense(\n",
    "                  self._hidden_size * self._num_heads, use_bias=False))\n",
    "            self._layers['K'].append(\n",
    "              tf.keras.layers.Dense(\n",
    "                  self._hidden_size * self._num_heads, use_bias=False))\n",
    "            self._layers['V'].append(\n",
    "              tf.keras.layers.Dense(\n",
    "                  self._hidden_size * self._num_heads, use_bias=False))\n",
    "\n",
    "            if self._multihead_aggregation == 'concat':\n",
    "                self._layers['head_agg'].append(\n",
    "                tf.keras.layers.Dense(self._hidden_size, use_bias=False))\n",
    "\n",
    "            self._layers['ffn'].append([])\n",
    "          # Don't need relu for the last feedforward.\n",
    "            for _ in range(self._num_feedforward - 1):\n",
    "                self._layers['ffn'][i].append(\n",
    "                tf.keras.layers.Dense(self._hidden_size, activation='relu'))\n",
    "            self._layers['ffn'][i].append(tf.keras.layers.Dense(self._hidden_size))\n",
    "\n",
    "    def feedforward(self, features, stack_index, training=None):\n",
    "        \"\"\"Feedforward component of Transformer.\n",
    "\n",
    "        Args:\n",
    "          features: 3D float Tensor of size (batch_size, num_features,\n",
    "            embedding_size). This is the input embedding to GCT.\n",
    "          stack_index: An integer to indicate which Transformer block we are in.\n",
    "          training: Whether to run in training or eval mode.\n",
    "\n",
    "        Returns:\n",
    "          Latent representations derived from this feedforward network.\n",
    "        \"\"\"\n",
    "        for i in range(self._num_feedforward):\n",
    "            features = self._layers['ffn'][stack_index][i](features)\n",
    "            if training:\n",
    "                features = tf.nn.dropout(features, rate=self._ffn_dropout)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def qk_op(self,\n",
    "            features,\n",
    "            stack_index,\n",
    "            batch_size,\n",
    "            num_codes,\n",
    "            attention_mask,\n",
    "            inf_mask=None,\n",
    "            directed_mask=None):\n",
    "        \"\"\"Attention generation part of Transformer.\n",
    "\n",
    "        Args:\n",
    "          features: 3D float Tensor of size (batch_size, num_features,\n",
    "            embedding_size). This is the input embedding to GCT.\n",
    "          stack_index: An integer to indicate which Transformer block we are in.\n",
    "          batch_size: The size of the mini batch.\n",
    "          num_codes: The number of features (i.e. codes) given as input.\n",
    "          attention_mask: A Tensor for suppressing the attention on the padded\n",
    "            tokens.\n",
    "          inf_mask: The guide matrix to suppress the attention values to zeros for\n",
    "            certain parts of the attention matrix (e.g. diagnosis codes cannot\n",
    "            attend to other diagnosis codes).\n",
    "          directed_mask: If the user wants to only use the upper-triangle of the\n",
    "            attention for uni-directional attention flow, we use this strictly lower\n",
    "            triangular matrix filled with infinity.\n",
    "\n",
    "        Returns:\n",
    "          The attention distribution derived from the QK operation.\n",
    "        \"\"\"\n",
    "\n",
    "        q = self._layers['Q'][stack_index](features)\n",
    "        q = tf.reshape(q,\n",
    "                       [batch_size, num_codes, self._hidden_size, self._num_heads])\n",
    "\n",
    "        k = self._layers['K'][stack_index](features)\n",
    "        k = tf.reshape(k,\n",
    "                       [batch_size, num_codes, self._hidden_size, self._num_heads])\n",
    "\n",
    "        # Need to transpose q and k to (2, 0, 1)\n",
    "        q = tf.transpose(q, perm=[0, 3, 1, 2])\n",
    "        k = tf.transpose(k, perm=[0, 3, 2, 1])\n",
    "        pre_softmax = tf.matmul(q, k) / tf.sqrt(\n",
    "            tf.cast(self._hidden_size, tf.float32))\n",
    "\n",
    "        pre_softmax -= attention_mask[:, None, None, :]\n",
    "\n",
    "        if inf_mask is not None:\n",
    "            pre_softmax -= inf_mask[:, None, :, :]\n",
    "\n",
    "        if directed_mask is not None:\n",
    "            pre_softmax -= directed_mask\n",
    "\n",
    "        if self._attention_normalizer == 'softmax':\n",
    "            attention = tf.nn.softmax(pre_softmax, axis=3)\n",
    "        else:\n",
    "            attention = tf.nn.sigmoid(pre_softmax)\n",
    "        return attention\n",
    "\n",
    "    def call(self, features,masks, guide=None, prior_guide=None, training=None):\n",
    "        \"\"\"Transforms input embeddings to output embeddings with attention mechanism.\"\"\"\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        num_codes = tf.shape(features)[1]\n",
    "\n",
    "        # Use masks to create an attention mask\n",
    "        mask_idx = tf.cast(tf.where(tf.equal(masks[:, :, 0], 0)), tf.int32)\n",
    "        attention_mask = tf.fill(tf.shape(features)[:-1], -1e9)  # Use large negative to simulate -inf\n",
    "        attention_mask = tf.tensor_scatter_nd_update(attention_mask, mask_idx, tf.zeros_like(mask_idx[:, 0], dtype=tf.float32))\n",
    "\n",
    "        inf_mask = None\n",
    "        if self._use_inf_mask:\n",
    "            inf_mask = tf.cast(tf.equal(guide, 0), tf.float32) * -1e9\n",
    "\n",
    "        directed_mask = None\n",
    "        if self._directed_attention:\n",
    "            lower_triangular = tf.linalg.band_part(tf.ones((num_codes, num_codes)), -1, 0)\n",
    "            directed_mask = -1e9 * (1.0 - lower_triangular)\n",
    "\n",
    "        attentions = []\n",
    "        for i in range(self._num_stack):\n",
    "            if self._use_prior and i == 0:\n",
    "                attention_scores = tf.tile(prior_guide[:, None, :, :], [1, self._num_heads, 1, 1])\n",
    "            else:\n",
    "                attention_scores = self.qk_op(features, i, batch_size, num_codes, attention_mask, inf_mask, directed_mask)\n",
    "\n",
    "            attentions.append(attention_scores)\n",
    "\n",
    "            v = self._layers['V'][i](features)\n",
    "            v = tf.reshape(v, [batch_size, num_codes, self._hidden_size, self._num_heads])\n",
    "            v = tf.transpose(v, perm=[0, 3, 1, 2])\n",
    "            post_attention = tf.matmul(attention_scores, v)\n",
    "\n",
    "            if self._num_heads == 1:\n",
    "                post_attention = tf.squeeze(post_attention, axis=1)\n",
    "            elif self._multihead_aggregation == 'concat':\n",
    "                post_attention = tf.transpose(post_attention, perm=[0, 2, 1, 3])\n",
    "                post_attention = tf.reshape(post_attention, [batch_size, num_codes, -1])\n",
    "                post_attention = self._layers['head_agg'][i](post_attention)\n",
    "            else:\n",
    "                post_attention = tf.reduce_sum(post_attention, axis=1)\n",
    "\n",
    "            # Residual connection and layer normalization\n",
    "            post_attention += features\n",
    "            post_attention = self.layer_norm(post_attention)  # Apply LayerNormalization\n",
    "\n",
    "            # Feedforward network\n",
    "            post_ffn = self.feedforward(post_attention, i, training)\n",
    "            post_ffn += post_attention\n",
    "            post_ffn = self.layer_norm(post_ffn)  # Apply LayerNormalization again\n",
    "\n",
    "            features = post_ffn\n",
    "\n",
    "        return features * masks, attentions\n",
    "\n",
    "\n",
    "\n",
    "def create_matrix_vdp(features, mask, use_prior, use_inf_mask, max_num_codes, prior_scalar):\n",
    "    dx_ids = features['dx_ints']\n",
    "    proc_ids = features['proc_ints']\n",
    "\n",
    "    batch_size = dx_ids.dense_shape[0]\n",
    "    num_dx_ids = max_num_codes if use_prior else dx_ids.dense_shape[-1]\n",
    "    num_proc_ids = max_num_codes if use_prior else proc_ids.dense_shape[-1]\n",
    "    num_codes = 1 + num_dx_ids + num_proc_ids\n",
    "\n",
    "    guide = None\n",
    "    if use_inf_mask:\n",
    "        row0 = tf.concat([\n",
    "            tf.zeros([1, 1]),\n",
    "            tf.ones([1, num_dx_ids]),\n",
    "            tf.zeros([1, num_proc_ids])\n",
    "        ], axis=1)\n",
    "\n",
    "        row1 = tf.concat([\n",
    "            tf.zeros([num_dx_ids, 1 + num_dx_ids]),\n",
    "            tf.ones([num_dx_ids, num_proc_ids])\n",
    "        ], axis=1)\n",
    "\n",
    "        row2 = tf.zeros([num_proc_ids, num_codes])\n",
    "\n",
    "        guide = tf.concat([row0, row1, row2], axis=0)\n",
    "        guide = guide + tf.transpose(guide)\n",
    "        guide = tf.tile(guide[None, :, :], [batch_size, 1, 1])\n",
    "        guide *= mask[:, :, None] * mask[:, None, :]\n",
    "        guide += tf.eye(num_codes)[None, :, :]\n",
    "\n",
    "    prior_guide = None\n",
    "    if use_prior:\n",
    "        prior_values = features['prior_values']\n",
    "        prior_indices = features['prior_indices']\n",
    "        prior_batch_idx = prior_indices.indices[:, 0][::2]\n",
    "        prior_idx = tf.reshape(prior_indices.values, [-1, 2])\n",
    "        prior_idx = tf.concat(\n",
    "            [prior_batch_idx[:, None], prior_idx[:, :1], prior_idx[:, 1:]], axis=1)\n",
    "\n",
    "        temp_idx = (\n",
    "            prior_idx[:, 0] * 1000000 + prior_idx[:, 1] * 1000 + prior_idx[:, 2])\n",
    "        sorted_idx = tf.argsort(temp_idx)\n",
    "        prior_idx = tf.gather(prior_idx, sorted_idx)\n",
    "\n",
    "        prior_idx_shape = [batch_size, max_num_codes * 2, max_num_codes * 2]\n",
    "        sparse_prior = tf.SparseTensor(\n",
    "            indices=prior_idx, values=prior_values.values, dense_shape=prior_idx_shape)\n",
    "        prior_guide = tf.sparse.to_dense(sparse_prior, default_value=0)\n",
    "\n",
    "        visit_guide = tf.convert_to_tensor(\n",
    "            [prior_scalar] * max_num_codes + [0.0] * max_num_codes,\n",
    "            dtype=tf.float32)\n",
    "        prior_guide = tf.concat(\n",
    "            [tf.tile(visit_guide[None, None, :], [batch_size, 1, 1]), prior_guide],\n",
    "            axis=1)\n",
    "        visit_guide = tf.concat([[0.0], visit_guide], axis=0)\n",
    "        prior_guide = tf.concat(\n",
    "            [tf.tile(visit_guide[None, :, None], [batch_size, 1, 1]), prior_guide],\n",
    "            axis=2)\n",
    "        prior_guide *= mask[:, :, None] * mask[:, None, :]\n",
    "        prior_guide += prior_scalar * tf.eye(num_codes)[None, :, :]\n",
    "        degrees = tf.reduce_sum(prior_guide, axis=2)\n",
    "        prior_guide /= degrees[:, :, None]\n",
    "\n",
    "    return guide, prior_guide\n",
    "\n",
    "\n",
    "\n",
    "class SequenceExampleParser(object):\n",
    "    def __init__(self, batch_size, num_map_threads=4):\n",
    "        self.context_features_config = {\n",
    "            'patientId': tf.io.VarLenFeature(dtype=tf.string),\n",
    "            'label.readmission': tf.io.FixedLenFeature(shape=[1], dtype=tf.int64),\n",
    "            'label.expired': tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "        }\n",
    "        self.sequence_features_config = {\n",
    "            'dx_ints': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "            'proc_ints': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "            'prior_indices': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "            'prior_values': tf.io.VarLenFeature(dtype=tf.float32)\n",
    "        }\n",
    "        self.batch_size = batch_size\n",
    "        self.num_map_threads = num_map_threads\n",
    "\n",
    "    def __call__(self, tfrecord_path, label_key, training):\n",
    "        def parser_fn(serialized_example):\n",
    "            (batch_context, batch_sequence) = tf.io.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                context_features=self.context_features_config,\n",
    "                sequence_features=self.sequence_features_config)\n",
    "            labels = tf.squeeze(tf.cast(batch_context[label_key], tf.float32))\n",
    "            return batch_sequence, labels\n",
    "\n",
    "        num_epochs = None if training else 1\n",
    "        buffer_size = self.batch_size * 32\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "        dataset = dataset.shuffle(buffer_size) if training else dataset\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.map(parser_fn, num_parallel_calls=self.num_map_threads)\n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "\n",
    "class EHRTransformer(object):\n",
    "    \"\"\"Transformer-based EHR encounter modeling algorithm.\n",
    "\n",
    "    All features within each encounter are put through multiple steps of\n",
    "    self-attention. There is a dummy visit embedding in addition to other\n",
    "    feature embeddings, which can be used for encounter-level predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               gct_params,\n",
    "               feature_keys=['dx_ints', 'proc_ints'],\n",
    "               label_key='label.readmission',\n",
    "               vocab_sizes={'dx_ints':3249, 'proc_ints':2210},\n",
    "               feature_set='vdp',\n",
    "               max_num_codes=50,\n",
    "               prior_scalar=0.5,\n",
    "               reg_coef=0.1,\n",
    "               num_classes=1,\n",
    "               learning_rate=1e-3,\n",
    "               batch_size=32):\n",
    "        \"\"\"Init function.\n",
    "\n",
    "        Args:\n",
    "          gct_params: A dictionary parameteres to be used inside GCT class. See GCT\n",
    "            comments for more information.\n",
    "          feature_keys: A list of feature names you want to use. (e.g. ['dx_ints,\n",
    "            'proc_ints', 'lab_ints'])\n",
    "          vocab_sizes: A dictionary of vocabularize sizes for each feature. (e.g.\n",
    "            {'dx_ints': 1001, 'proc_ints': 1001, 'lab_ints': 1001})\n",
    "          feature_set: Use 'vdpl' to indicate your features are diagnosis codes,\n",
    "            treatment codes, and lab codes. Use 'vdp' to indicate your features are\n",
    "            diagnosis codes and treatment codes.\n",
    "          max_num_codes: The maximum number of how many feature there can be inside\n",
    "            a single visit, per feature. For example, if this is set to 50, then we\n",
    "            are assuming there can be up to 50 diagnosis codes, 50 treatment codes,\n",
    "            and 50 lab codes. This will be used for creating the prior matrix.\n",
    "          prior_scalar: A float value between 0.0 and 1.0 to be used to hard-code\n",
    "            the diagnoal elements of the prior matrix.\n",
    "          reg_coef: A coefficient to decide the KL regularization balance when\n",
    "            training GCT.\n",
    "          num_classes: This is set to 1, because this implementation only supports\n",
    "            graph-level binary classification.\n",
    "          learning_rate: Learning rate for Adam optimizer.\n",
    "          batch_size: Batch size.\n",
    "        \"\"\"\n",
    "        self._feature_keys = feature_keys\n",
    "        self._label_key = label_key\n",
    "        self._vocab_sizes = vocab_sizes\n",
    "        self._feature_set = feature_set\n",
    "        self._max_num_codes = max_num_codes\n",
    "        self._prior_scalar = prior_scalar\n",
    "        self._reg_coef = reg_coef\n",
    "        self._num_classes = num_classes\n",
    "        self._learning_rate = learning_rate\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._gct_params = gct_params\n",
    "        self._embedding_size = gct_params['embedding_size']\n",
    "        self._num_transformer_stack = gct_params['num_transformer_stack']\n",
    "        self._use_inf_mask = gct_params['use_inf_mask']\n",
    "        self._use_prior = gct_params['use_prior']\n",
    "\n",
    "        self._seqex_reader = SequenceExampleParser(self._batch_size)\n",
    "\n",
    "        self.dense_layer = tf.keras.layers.Dense(self._num_classes, activation=None)\n",
    "\n",
    "        self.loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    def get_prediction(self, model, feature_embedder, features, training=False):\n",
    "\n",
    "        embedding_dict, mask_dict = feature_embedder.lookup(features, self._max_num_codes)\n",
    "        keys = ['visit'] + self._feature_keys\n",
    "        embeddings = tf.concat([embedding_dict[key] for key in keys], axis=1)\n",
    "\n",
    "        #print([mask_dict[key].shape for key in keys])\n",
    "\n",
    "        masks = tf.concat([mask_dict[key] for key in keys], axis=1)\n",
    "\n",
    "        # Assuming create_matrix_vdp is updated for TensorFlow 2.x\n",
    "        guide, prior_guide = create_matrix_vdp(features, masks, self._use_prior,\n",
    "                                               self._use_inf_mask, self._max_num_codes,\n",
    "                                               self._prior_scalar)\n",
    "\n",
    "        hidden, attentions = model(embeddings, masks[:, :, None], guide, prior_guide, training)\n",
    "\n",
    "        pre_logit = hidden[:, 0, :]\n",
    "        pre_logit = tf.reshape(pre_logit, [-1, self._embedding_size])\n",
    "        logits = self.dense_layer(pre_logit)\n",
    "        logits = tf.squeeze(logits)\n",
    "\n",
    "        return logits, attentions\n",
    "\n",
    "    def get_loss(self, logits, labels, attentions):\n",
    "        loss = self.loss_function(labels, logits)\n",
    "\n",
    "        if self._use_prior:\n",
    "            kl_terms = []\n",
    "            attention_tensor = tf.stack(attentions)\n",
    "            for i in range(1, self._num_transformer_stack):\n",
    "                log_p = tf.math.log(attention_tensor[i - 1] + 1e-12)\n",
    "                log_q = tf.math.log(attention_tensor[i] + 1e-12)\n",
    "                kl_term = attention_tensor[i - 1] * (log_p - log_q)\n",
    "                kl_term = tf.reduce_sum(kl_term, axis=-1)  # Sum across the last dimension\n",
    "                kl_term = tf.reduce_mean(kl_term)  # Mean across the batch\n",
    "                kl_terms.append(kl_term)\n",
    "\n",
    "            reg_term = tf.reduce_mean(kl_terms)  # Mean across all layers\n",
    "            loss += self._reg_coef * reg_term\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def input_fn(self, tfrecord_path, training):\n",
    "        \"\"\"Input function to be used by TensorFlow Estimator.\n",
    "\n",
    "        Args:\n",
    "          tfrecord_path: Path to TFRecord of SequenceExamples.\n",
    "          training: Boolean value to indicate whether the model if training.\n",
    "\n",
    "        Return:\n",
    "          Input generator.\n",
    "        \"\"\"\n",
    "        return self._seqex_reader(tfrecord_path, self._label_key, training)\n",
    "\n",
    "\n",
    "    def model_fn(self, features, labels, mode, params):\n",
    "        training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "        # Initialize model and feature embedder\n",
    "        model = GraphConvolutionalTransformer(**self._gct_params)\n",
    "        feature_embedder = FeatureEmbedder(\n",
    "            self._vocab_sizes, self._feature_keys, self._embedding_size)\n",
    "\n",
    "        logits, attentions = self.get_prediction(model, feature_embedder, features, training)\n",
    "        probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "        predictions = {\n",
    "            'probabilities': probs,\n",
    "            'logits': logits,\n",
    "        }\n",
    "\n",
    "        # Output predictions in PREDICT mode\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = get_loss(logits, labels, attentions)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=self._learning_rate)\n",
    "            train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            # Define the metrics\n",
    "            metrics_dict = {\n",
    "                'AUC-PR': tf.keras.metrics.AUC(labels, probs, curve='PR'),\n",
    "                'AUC-ROC': tf.keras.metrics.AUC(labels, probs, curve='ROC')\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0Q-Lbe3uUDx"
   },
   "outputs": [],
   "source": [
    "\"\"\"Copyright 2019 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "def main():\n",
    "    gct_params = {\n",
    "      \"embedding_size\": 128,\n",
    "      \"num_transformer_stack\": 3,\n",
    "      \"num_feedforward\": 2,\n",
    "      \"num_attention_heads\": 1,\n",
    "      \"ffn_dropout\": 0.08,\n",
    "      \"attention_normalizer\": \"softmax\",\n",
    "      \"multihead_attention_aggregation\": \"concat\",\n",
    "      \"directed_attention\": False,\n",
    "      \"use_inf_mask\": True,\n",
    "      \"use_prior\": True,\n",
    "    }\n",
    "\n",
    "    input_path = '/output/fold_0'\n",
    "    model_dir = current_dir+'/result'\n",
    "    #   num_iter = 1000000\n",
    "    num_iter = 4800  # 2 hours for 100 per 115 second in my mac.\n",
    "    model = EHRTransformer(\n",
    "      gct_params=gct_params,\n",
    "      label_key='label.readmission',\n",
    "      reg_coef=0.1,\n",
    "      learning_rate=0.00022,\n",
    "      batch_size=32)\n",
    "    config = tf.estimator.RunConfig(save_checkpoints_steps=100)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "      model_dir=model_dir, model_fn=model.model_fn, config=config)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "      input_fn=lambda: model.input_fn(input_path + '/train.tfrecord', True),\n",
    "      max_steps=num_iter)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "      input_fn=lambda: model.input_fn(input_path + '/validation.tfrecord', False),\n",
    "      throttle_secs=1)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "    estimator.evaluate(\n",
    "      input_fn=lambda: model.input_fn(input_path + '/validation.tfrecord', False))\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.INFO)\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results\n",
    "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
    "\n",
    "Please test and report results for all experiments that you run with:\n",
    "\n",
    "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
    "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [],
   "source": [
    "# metrics to evaluate my model\n",
    "print('run in my local python2.7 and test by using the test file, It gives the following result:')\n",
    "print(\"Test results: {'AUC-ROC': 0.5, 'AUC-PR': 0.16597612, 'global_step': 1000, 'loss': 0.6367319}\")\n",
    "# plot figures to better show the results\n",
    "\n",
    "# it is better to save the numbers and figures for your presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "outputs": [],
   "source": [
    "# compare you model with others\n",
    "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper\n",
    "print(\"The above result 0.16597612 is much worse than the paper's result 0.5965 and all other models\")\n",
    "print(\"This is due to the less training time and other selected parameter like learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
    "  * Make assessment that the paper is reproducible or not.\n",
    "  * Explain why it is not reproducible if your results are kind negative.\n",
    "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
    "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
    "  * What will you do in next phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2VDXo5F4Frm"
   },
   "outputs": [],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can read and plot it here like the Scope of Reproducibility\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Learning the Graphical Structure of Electronic Health Records with Graph Convolutional Transformer\n",
    "Edward Choi, Zhen Xu, Yujia Li, Michael W. Dusenberry, Gerardo Flores, Yuan Xue, Andrew M. Dai  \n",
    "AAAI 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmVuzQ724HbO"
   },
   "source": [
    "# Feel free to add new sections"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
