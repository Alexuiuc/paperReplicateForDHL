{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672e31b3-12e8-4126-ac17-4ab136de68ff",
   "metadata": {},
   "source": [
    "# Reproduction of Paper `Learning the Graphical Structure of Electronic Health Records with Graph Convolutional Transformer` by DL4H Team 137 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840fd296-5239-4bb5-b104-5c32a3f015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ pip freeze > requirements.txt\n",
    "# $ conda env export > environment.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c6f15-fa4e-4896-b74c-6dad6e775338",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887be03-ecdb-4c3d-b7ae-141d39988433",
   "metadata": {},
   "source": [
    "*   Background of the problem\n",
    "    * This study focuses on readmission/mortality prediction.\n",
    "    * Unstructured data, particularly claims data, lacks a clear structure, making it challenging for models like MiME (Choi et al., 2018) to be utilized effectively.\n",
    "    * The primary difficulties include discovering the hidden structure of the data while simultaneously making predictions.\n",
    "\n",
    "    * The approach outlined in the paper is effective according to their test metrics.\n",
    "*   Paper explanation\n",
    "    * The paper proposes a new method, the Graph Convolutional Transformer (GCT), to jointly learn the hidden structure and perform the prediction task. This method uses unstructured data as the initial input and achieves accurate predictions for general medical tasks.\n",
    "\n",
    "    * TEST METRICS FROM THE PAPER ARE SHOWN BELOW\n",
    "    * It offers significant benefits for individuals without access to structured data. Additionally, the learned structure can be useful for others who wish to reuse the learned structure for future studies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56976b-a2dd-480c-b702-64588ada58d4",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9f946-aa6d-4c5d-8d96-80702ae4dc1b",
   "metadata": {},
   "source": [
    "# Scope of reproducibility (5)\n",
    "The scope of this reproducibility study focuses on verifying the results claimed in the paper \"Learning the Graphical Structure of Electronic Health Records with Graph Convolutional Transformer\". The goal is to reproduce the model's ability to predict readmission/mortality using electronic health records as described in the original research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075473a-4b6e-4c00-8b31-d12cdd72e24d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704da98b-3204-411f-ac03-fab2451f1258",
   "metadata": {},
   "source": [
    "# Methodology (15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59a320-f803-4ad4-a9b3-44374ccce701",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df6de871-b2c6-4630-8f0c-ec0048be299b",
   "metadata": {},
   "source": [
    "## Environment\n",
    "### Python version\n",
    "- Python 3.10\n",
    "### Dependencies/packages needed\n",
    "- torch==1.7.1\n",
    "- numpy==1.19.5\n",
    "- pandas==1.2.0\n",
    "- scikit-learn==0.24.1\n",
    "- matplotlib==3.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc33ad1-916b-4750-ab0b-23392f2a5d70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73a90915-9ec9-421f-9356-d4b786ee5724",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Data download instruction\n",
    "- Data can be downloaded from `[Insert Link Here]`.\n",
    "### Data descriptions with helpful charts and visualizations\n",
    "- Include pie charts and histograms of key demographics and clinical features.\n",
    "### Preprocessing code + command\n",
    "- `python preprocessing.py --input path/to/raw/data --output path/to/cleaned/data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbded87-edf4-4de0-a338-3970d8f6f299",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13af20f8-1e28-40a8-893a-a00ddb6f7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filenames = ['patient', 'admissionDx', 'diagnosis', 'treatment']\n",
    "raw_data_dir='./'\n",
    "SUBSET_RATIO=1.0\n",
    "\n",
    "def load_raw_data(raw_data_dir, filenames, subset_ratio=SUBSET_RATIO):\n",
    "\n",
    "    data_frames = {}\n",
    "    for filename in filenames:\n",
    "        file_path = raw_data_dir + filename +'.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if subset_ratio < 1.0:\n",
    "            df = df.sample(frac=subset_ratio)\n",
    "        data_frames[filename]=df\n",
    "    return data_frames\n",
    "data_frames = load_raw_data(raw_data_dir, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458c977-096e-4f2a-888b-10e42255d2e5",
   "metadata": {},
   "source": [
    "#### Preprocess data and generate the eicu record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e0cbc5-39e8-40d7-a15e-9b4dd5985541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# preprocess patient data\n",
    "def process_patient(df, hour_threshold=24):\n",
    "    # Calculate encounter_timestamp and create a temporary DataFrame for sorting\n",
    "    if df['patientunitstayid'].duplicated().any():      \n",
    "        print('Duplicate encounter ID!!')\n",
    "        sys.exit(0)\n",
    "    df['encounter_timestamp'] = -df['hospitaladmitoffset'].astype(int)\n",
    "\n",
    "    # Sorting patients by their IDs and then by the encounter timestamp\n",
    "    df_sorted = df.sort_values(['patienthealthsystemstayid', 'encounter_timestamp'])\n",
    "\n",
    "    # Detect readmissions by checking if the next stay is within the same patient ID\n",
    "    \n",
    "    df_sorted['readmission'] = True  # Initially mark all as True\n",
    "    df_sorted.loc[df_sorted.groupby('patienthealthsystemstayid')['patientunitstayid'].tail(1).index, 'readmission'] = False\n",
    "    df_sorted['unitdischargestatus'] = df_sorted['unitdischargestatus']=='Expired'\n",
    "\n",
    "    duration_threshold = hour_threshold * 60.0\n",
    "    mask = df_sorted['unitdischargeoffset'] <= duration_threshold\n",
    "    \n",
    "    df_sorted = df_sorted[mask]\n",
    "    rename_dict = {'patienthealthsystemstayid':'patient_id',\n",
    "                   'patientunitstayid':'encounter_id',\n",
    "                   'encounter_timestamp':'encounter_timestamp',\n",
    "                   'unitdischargestatus':'expired',\n",
    "                  }\n",
    "    df_selected = df_sorted[ list(rename_dict.keys())+['readmission'] ]\n",
    "    df_renamed = df_selected.rename(columns=rename_dict)\n",
    "    return df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370b4287-a59c-46d2-a627-1c216a319293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "patient_dataframe = process_patient(data_frames['patient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2455f02f-fd82-4c91-9290-c95999821d63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>encounter_timestamp</th>\n",
       "      <th>expired</th>\n",
       "      <th>readmission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128927</td>\n",
       "      <td>141178</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>128943</td>\n",
       "      <td>141197</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128952</td>\n",
       "      <td>141208</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128970</td>\n",
       "      <td>141229</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>128995</td>\n",
       "      <td>141260</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    patient_id  encounter_id  encounter_timestamp  expired  readmission\n",
       "1       128927        141178                   14    False         True\n",
       "5       128943        141197                   25    False         True\n",
       "7       128952        141208                    1    False        False\n",
       "9       128970        141229                    4    False        False\n",
       "12      128995        141260                   18    False        False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_dataframe[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fefae76b-94c1-4f54-8528-ed8a947145bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test correctness\n",
    "\n",
    "# patient_dataframe[patient_dataframe['readmission']==True][:5]\n",
    "# pdf = data_frames['patient']\n",
    "# pdf[ pdf['patienthealthsystemstayid']==133737]\n",
    "# patient_dataframe[pdf['patientunitstayid']==147378]['readmission']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a76c6ea-4d79-4290-be44-5ac3f013bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admission without Encounter ID: 450589\n"
     ]
    }
   ],
   "source": [
    "# process admission\n",
    "def process_admission_dx(df,patient_df):\n",
    "    # Check and report the number of missing encounter IDs\n",
    "    \n",
    "    df['admitdxpath'] = df['admitdxpath'].str.lower()\n",
    "    \n",
    "    patient_encounter_ids = set(patient_df['encounter_id'])\n",
    "\n",
    "    mask = df['patientunitstayid'].isin(patient_encounter_ids)\n",
    "\n",
    "    missing_eid = df[~mask]\n",
    "    \n",
    "    print('admission without Encounter ID:', len(missing_eid))\n",
    "    \n",
    "    df = df[mask]\n",
    "    rename_dict = {'patientunitstayid':'encounter_id',\n",
    "                   'admitdxpath':'dx_id'\n",
    "                  }\n",
    "    df_selected = df[list(rename_dict.keys()) ]\n",
    "    df_renamed = df_selected.rename(columns=rename_dict)\n",
    "    \n",
    "    return df_renamed\n",
    "\n",
    "admission_dataframe = process_admission_dx(data_frames['admissionDx'],patient_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d8fdec-c8f2-4f26-a06a-b99f4b1cbcbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>dx_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2900366</td>\n",
       "      <td>admission diagnosis|was the patient admitted f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2900366</td>\n",
       "      <td>admission diagnosis|all diagnosis|non-operativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2900366</td>\n",
       "      <td>admission diagnosis|non-operative organ system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2900423</td>\n",
       "      <td>admission diagnosis|non-operative organ system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2900423</td>\n",
       "      <td>admission diagnosis|was the patient admitted f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    encounter_id                                              dx_id\n",
       "26       2900366  admission diagnosis|was the patient admitted f...\n",
       "27       2900366  admission diagnosis|all diagnosis|non-operativ...\n",
       "28       2900366  admission diagnosis|non-operative organ system...\n",
       "36       2900423  admission diagnosis|non-operative organ system...\n",
       "37       2900423  admission diagnosis|was the patient admitted f..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission_dataframe[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8790fc-d05d-4a0f-b354-4e06f8f74e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission Diagnosis without Encounter ID: 2483092\n"
     ]
    }
   ],
   "source": [
    "def process_diagnosis(df,patient_df):\n",
    "    # Check and report the number of missing encounter IDs\n",
    "    \n",
    "    df['diagnosisstring'] = df['diagnosisstring'].str.lower()\n",
    "    \n",
    "    patient_encounter_ids = set(patient_df['encounter_id'])\n",
    "\n",
    "    mask = df['patientunitstayid'].isin(patient_encounter_ids)\n",
    "\n",
    "    missing_eid = df[~mask]\n",
    "    \n",
    "    print('Admission Diagnosis without Encounter ID:', len(missing_eid))\n",
    "    \n",
    "    df = df[mask]\n",
    "    rename_dict = {'patientunitstayid':'encounter_id',\n",
    "                   'diagnosisstring':'dx_id'\n",
    "                  }\n",
    "    df_selected = df[list(rename_dict.keys()) ]\n",
    "    df_renamed = df_selected.rename(columns=rename_dict)\n",
    "    \n",
    "    return df_renamed\n",
    "diagnosis_dataframe = process_diagnosis(data_frames['diagnosis'],patient_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f66d86-f620-40bc-b07c-78bd3ba55c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>dx_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>141229</td>\n",
       "      <td>cardiovascular|arrhythmias|atrial fibrillation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>141229</td>\n",
       "      <td>cardiovascular|ventricular disorders|acute pul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>141229</td>\n",
       "      <td>cardiovascular|ventricular disorders|congestiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>141229</td>\n",
       "      <td>neurologic|altered mental status / pain|change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>141229</td>\n",
       "      <td>cardiovascular|ventricular disorders|acute pul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    encounter_id                                              dx_id\n",
       "30        141229     cardiovascular|arrhythmias|atrial fibrillation\n",
       "31        141229  cardiovascular|ventricular disorders|acute pul...\n",
       "32        141229  cardiovascular|ventricular disorders|congestiv...\n",
       "33        141229  neurologic|altered mental status / pain|change...\n",
       "34        141229  cardiovascular|ventricular disorders|acute pul..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis_dataframe[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5e0344-4653-4255-b7a2-c93ea5ea15b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment without Encounter ID: 3372000\n"
     ]
    }
   ],
   "source": [
    "def process_treatment(df, patient_df):\n",
    "    \n",
    "    df['treatmentstring'] = df['treatmentstring'].str.lower()\n",
    "    \n",
    "    patient_encounter_ids = set(patient_df['encounter_id'])\n",
    "\n",
    "    mask = df['patientunitstayid'].isin(patient_encounter_ids)\n",
    "\n",
    "    missing_eid = df[~mask]\n",
    "    \n",
    "    print('treatment without Encounter ID:', len(missing_eid))\n",
    "    \n",
    "    df = df[mask]\n",
    "    rename_dict = {'patientunitstayid':'encounter_id',\n",
    "                   'treatmentstring':'treatment'\n",
    "                  }\n",
    "\n",
    "    df_selected = df[list(rename_dict.keys()) ]\n",
    "    df_renamed = df_selected.rename(columns=rename_dict)\n",
    "    \n",
    "    return df_renamed\n",
    "\n",
    "treatment_dataframe = process_treatment(data_frames['treatment'], patient_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8e1cd3-5c79-4edc-b2db-0dde99fccbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>242203</td>\n",
       "      <td>gastrointestinal|medications|stress ulcer prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>242203</td>\n",
       "      <td>pulmonary|ventilation and oxygenation|oxygen t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>242203</td>\n",
       "      <td>renal|urinary catheters|foley catheter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>242203</td>\n",
       "      <td>renal|electrolyte correction|treatment of hype...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>242203</td>\n",
       "      <td>gastrointestinal|medications|antiemetic|seroto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     encounter_id                                          treatment\n",
       "224        242203  gastrointestinal|medications|stress ulcer prop...\n",
       "225        242203  pulmonary|ventilation and oxygenation|oxygen t...\n",
       "226        242203             renal|urinary catheters|foley catheter\n",
       "227        242203  renal|electrolyte correction|treatment of hype...\n",
       "228        242203  gastrointestinal|medications|antiemetic|seroto..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_dataframe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73384064-4a34-48fb-babb-31cfd1c61e41",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e26ac93-bb64-4c80-898a-0b81629c1983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'double check ok'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"double check ok\"\"\"\n",
    "# output from python 2.7 environment\n",
    "# Processing patient.csv\n",
    "# Processing admission diagnosis.csv\n",
    "# Admission Diagnosis without Encounter ID: 450589\n",
    "# Processing diagnosis.csv\n",
    "# Diagnosis without Encounter ID: 2483092\n",
    "# Processing treatment.csv\n",
    "# Treatment without Encounter ID: 3372000\n",
    "# Accepted treatments: 316745\n",
    "\n",
    "# This is the same as above log output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5628db73-6b3d-4a7e-86e9-1ba0bb6961e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68076 176269 227580 316745\n"
     ]
    }
   ],
   "source": [
    "print(len(patient_dataframe),len(admission_dataframe),len(diagnosis_dataframe),len(treatment_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50f3f3a3-b05d-4d39-bf9f-b89385b03158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The content in sequence seqex_list'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The content in sequence seqex_list'\n",
    "# Context Features:\n",
    "# key: label.expired value: int64_list {\n",
    "#   value: 0\n",
    "# }\n",
    "\n",
    "# key: label.readmission value: int64_list {\n",
    "#   value: 0\n",
    "# }\n",
    "\n",
    "# key: patientId value: bytes_list {\n",
    "#   value: \"2630449:3229400\"\n",
    "# }\n",
    "\n",
    "# Feature Lists:\n",
    "# key: proc_ids\n",
    "# feature: bytes_list {\n",
    "#   value: \"pulmonary|ventilation and oxygenation|oxygen therapy (< 40%)|nasal cannula\"\n",
    "#   value: \"cardiovascular|intravenous fluid|normal saline administration\"\n",
    "#   value: \"endocrine|glucose metabolism|insulin|continuous infusion\"\n",
    "#   value: \"gastrointestinal|medications|stress ulcer prophylaxis|famotidine\"\n",
    "#   value: \"cardiovascular|arrhythmias|anticoagulant administration|low molecular weight heparin|enoxaparin\"\n",
    "# }\n",
    "\n",
    "# key: dx_ints\n",
    "# feature: int64_list {\n",
    "#   value: 202\n",
    "#   value: 0\n",
    "#   value: 201\n",
    "#   value: 164\n",
    "# }\n",
    "\n",
    "# key: dx_ids\n",
    "# feature: bytes_list {\n",
    "#   value: \"endocrine|glucose metabolism|dka\"\n",
    "#   value: \"admission diagnosis|was the patient admitted from the o.r. or went to the o.r. within 4 hours of admission?|no\"\n",
    "#   value: \"admission diagnosis|all diagnosis|non-operative|diagnosis|metabolic/endocrine|diabetic ketoacidosis\"\n",
    "#   value: \"admission diagnosis|non-operative organ systems|organ system|metabolic/endocrine\"\n",
    "# }\n",
    "\n",
    "# key: proc_ints\n",
    "# feature: int64_list {\n",
    "#   value: 68\n",
    "#   value: 27\n",
    "#   value: 273\n",
    "#   value: 80\n",
    "#   value: 417\n",
    "# }\n",
    "\n",
    "# first key: 1392393:1774519\n",
    "# content of dx_str2int:\n",
    "# key: surgery|respiratory failure|ventilatory failure|suspected value: 2762\n",
    "# key: burns/trauma|trauma-other injuries|traumatic amputation|arm/hand value: 3196\n",
    "# key: endocrine|fluids and electrolytes|hypernatremia|moderate (146 - 155 meq/dl) value: 1284\n",
    "# key: admission diagnosis|all diagnosis|operative|diagnosis|cardiovascular|thrombectomy (with general anesthesia) value: 975\n",
    "# key: endocrine|fluids and electrolytes|hyponatremia|severe (< 125 meq/dl) value: 709\n",
    "# content of treat_str2int:\n",
    "# key: pulmonary|surgery / incision and drainage of thorax|pulmonary resection|lobectomy value: 1083\n",
    "# key: neurologic|ich/ cerebral infarct|anticonvulsants|phenytoin value: 603\n",
    "# key: cardiovascular|arrhythmias|digoxin value: 284\n",
    "# key: toxicology|drug overdose|agent specific therapy|beta blockers overdose|atropine value: 1489\n",
    "# key: oncology|medications|analgesics|oral analgesics value: 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f78ac0d9-defd-4f14-959e-0c7df3fe0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(patient_dataframe, treatment_dataframe,diagnosis_dataframe,admission_dataframe, min_num_codes=1,\n",
    "                max_num_codes=50):\n",
    "\n",
    "    '''\n",
    "    This function is to bulid the dataframe for training,\n",
    "    it is equals to build_seqex in process_eicu.py\n",
    "    '''\n",
    "    filter = lambda x: len(x)>=min_num_codes and len(x)<=max_num_codes\n",
    "\n",
    "    # merge admission and diagnosis\n",
    "    merged_admission_diagnosis = pd.concat([admission_dataframe, diagnosis_dataframe], axis=0)\n",
    "\n",
    "    dx_list = list(set(merged_admission_diagnosis['dx_id']))\n",
    "\n",
    "    dx_str2int = {s:i for i,s in enumerate(dx_list)}\n",
    "\n",
    "    merged_admission_diagnosis['dx_ints'] = merged_admission_diagnosis['dx_id'].map(dx_str2int)\n",
    "    \n",
    "    merged_admission_diagnosis = merged_admission_diagnosis.groupby('encounter_id')['dx_ints'].agg(list).reset_index()\n",
    "\n",
    "    merged_admission_diagnosis =merged_admission_diagnosis[merged_admission_diagnosis['dx_ints'].apply(filter)]\n",
    "\n",
    "    # aggrigate treatment_dataframe\n",
    "    \n",
    "    treat_list = list(set(treatment_dataframe['treatment']))\n",
    "\n",
    "    treat_str2int = {s:i for i,s in enumerate(treat_list)}\n",
    "    \n",
    "    treatment_dataframe['proc_ints'] =  treatment_dataframe['treatment'].map(treat_str2int)\n",
    "    \n",
    "    treatment_dataframe = treatment_dataframe.groupby('encounter_id')['proc_ints'].agg(list).reset_index()\n",
    "\n",
    "    treatment_dataframe =treatment_dataframe[treatment_dataframe['proc_ints'].apply(filter)]\n",
    "    \n",
    "    #print(len(merged_admission_diagnosis),len(treatment_dataframe))\n",
    "    \n",
    "    # merge patient, admission and diagnosis\n",
    "    merged_patient_proc_ints = pd.merge(merged_admission_diagnosis, patient_dataframe, on='encounter_id', how='inner')\n",
    "\n",
    "    # merge patient, all\n",
    "\n",
    "    merged_df= pd.merge(merged_patient_proc_ints, treatment_dataframe, on='encounter_id', how='inner')\n",
    "    \n",
    "    merged_df['patientId'] = merged_df.apply(lambda row: (row['patient_id'], row['encounter_id']), axis=1)\n",
    "\n",
    "    merged_df.drop(['patient_id','encounter_timestamp'], axis=1, inplace=True)\n",
    "\n",
    "    merged_df.set_index('encounter_id', inplace=True)\n",
    "    \n",
    "    return  merged_df, dx_str2int, treat_str2int,dx_list,treat_list\n",
    "    \n",
    "df, dx_str2int, treat_str2int,dx_list,treat_list = build_dataframe(patient_dataframe, treatment_dataframe,diagnosis_dataframe,admission_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07630261-1955-493d-9b66-c85ce7abe330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40410\n"
     ]
    }
   ],
   "source": [
    "#TODO \n",
    "# Need to check the difference, \n",
    "# data process_eicu.py gives 41026\n",
    "# It may comes from the joining method and datatype\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f99c27-51ec-482a-88cf-ea2ef12af099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dx_ints        [2278, 125, 2067, 2130, 3300]\n",
       "expired                                False\n",
       "readmission                            False\n",
       "proc_ints                        [1002, 820]\n",
       "patientId                 (2743102, 3353254)\n",
       "Name: 3353254, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[3353254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16104871-f8d5-4384-861e-376388f9840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of the process_eicu.py for encounter Id: 3353254\n",
      "The number of the \"dx_ints\" and \"proc_ints\" is the same\n",
      "The value is different,which is ok, the value is just an \n",
      "index to dictionary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Output of the process_eicu.py for encounter Id: 3353254\n",
    "The number of the \"dx_ints\" and \"proc_ints\" is the same\n",
    "The value is different,which is ok, the value is just an \n",
    "index to dictionary\n",
    "\"\"\")\n",
    "# seqx for 3353254: context {\n",
    "#   feature {\n",
    "#     key: \"label.expired\"\n",
    "#     value {\n",
    "#       int64_list {\n",
    "#         value: 0\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature {\n",
    "#     key: \"label.readmission\"\n",
    "#     value {\n",
    "#       int64_list {\n",
    "#         value: 0\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature {\n",
    "#     key: \"patientId\"\n",
    "#     value {\n",
    "#       bytes_list {\n",
    "#         value: \"2743102:3353254\"\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "# feature_lists {\n",
    "#   feature_list {\n",
    "#     key: \"dx_ids\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         bytes_list {\n",
    "#           value: \"admission diagnosis|non-operative organ systems|organ system|gastrointestinal\"\n",
    "#           value: \"admission diagnosis|was the patient admitted from the o.r. or went to the o.r. within 4 hours of admission?|no\"\n",
    "#           value: \"renal|disorder of kidney|acute renal failure|due to hypovolemia/decreased circulating volume\"\n",
    "#           value: \"admission diagnosis|all diagnosis|non-operative|diagnosis|gastrointestinal|bleeding, lower gi\"\n",
    "#           value: \"gastrointestinal|gi bleeding / pud|lower gi bleeding\"\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature_list {\n",
    "#     key: \"dx_ints\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         int64_list {\n",
    "#           value: 31\n",
    "#           value: 0\n",
    "#           value: 225\n",
    "#           value: 323\n",
    "#           value: 324\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature_list {\n",
    "#     key: \"proc_ids\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         bytes_list {\n",
    "#           value: \"cardiovascular|intravenous fluid|normal saline administration|fluid bolus (250-1000mls)\"\n",
    "#           value: \"cardiovascular|intravenous fluid|blood product administration|packed red blood cells|transfusion of > 2 units prbc\\'s\"\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature_list {\n",
    "#     key: \"proc_ints\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         int64_list {\n",
    "#           value: 105\n",
    "#           value: 483\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72b0c0e5-7f0e-404c-8a50-fa061c441cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3351 2212\n"
     ]
    }
   ],
   "source": [
    "print(len(dx_str2int),len(treat_str2int))\n",
    "#TODO Check the difference. Code gives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c9ea53b-8fb3-44d6-9c06-eff0ef089324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01aec987-e658-46f4-bf67-d0d92abd30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_train_valid_test(df, target ='readmission', random_seed=0):\n",
    "    \n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=random_seed)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=random_seed)\n",
    "    \n",
    "    return train_df,valid_df,test_df\n",
    "train_df, validate_df, test_df = select_train_valid_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8f1461a-2c23-4f31-843a-2b66eba60aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def generate_combinations(row):\n",
    "    return list(product(row['dx_ints'], row['proc_ints']))\n",
    "\n",
    "total_visit = len(df)\n",
    "def count_conditional_prob_dp(df,total_visit):\n",
    "    \"\"\"\n",
    "    This is equals to the count_conditional_prob_dp in graph_convolutional_transformer.py\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    dx_explode = df['dx_ints'].explode()\n",
    "    dx_freqs = dx_explode.value_counts().to_dict()\n",
    "    proc_explode = df['proc_ints'].explode()\n",
    "    proc_freqs = proc_explode.value_counts().to_dict()\n",
    "    \n",
    "    df['dp'] = df.apply(generate_combinations, axis=1)\n",
    "    exploded_df = df.explode('dp')\n",
    "    dp_freqs = exploded_df['dp'].value_counts().to_dict()\n",
    "    \n",
    "    # print(dp_freqs)\n",
    "\n",
    "    dx_probs = dict([(k, v / float(total_visit)) for k, v in dx_freqs.items()])\n",
    "    proc_probs = dict([(k, v / float(total_visit)) for k, v in proc_freqs.items()])\n",
    "    dp_probs = dict([(k, v / float(total_visit)) for k, v in dp_freqs.items()])\n",
    "    \n",
    "    dp_cond_probs = {}\n",
    "    pd_cond_probs = {}\n",
    "    for dx, dx_prob in dx_probs.items():\n",
    "        for proc, proc_prob in proc_probs.items():\n",
    "            dp = tuple([dx, proc])\n",
    "            pd = tuple([proc, dx])\n",
    "            if dp in dp_probs:\n",
    "                dp_cond_probs[dp] = dp_probs[dp] / dx_prob\n",
    "                pd_cond_probs[pd] = dp_probs[dp] / proc_prob\n",
    "            else:\n",
    "                dp_cond_probs[dp] = 0.0\n",
    "                pd_cond_probs[pd] = 0.0\n",
    "    \n",
    "    return dx_probs, proc_probs, dp_probs, dp_cond_probs, pd_cond_probs\n",
    "    \n",
    "    \n",
    "dx_probs, proc_probs, dp_probs, dp_cond_probs, pd_cond_probs = count_conditional_prob_dp(train_df,total_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c84cf13-eed5-48c8-bb32-54b1c091705b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6381180, 6381180, 259759)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dp_cond_probs),len(pd_cond_probs),len(dp_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f698c5-7c2a-4fdd-a409-2e354fb5723a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((484, 125), 0.7152446564211271)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(pd_cond_probs.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c284538f-59cd-4c4e-b198-eefbb0a27868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx_ints</th>\n",
       "      <th>expired</th>\n",
       "      <th>readmission</th>\n",
       "      <th>proc_ints</th>\n",
       "      <th>patientId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encounter_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242203</th>\n",
       "      <td>[963, 3286, 125, 52, 978, 2866, 2866, 52, 978]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[1212, 1200, 1136, 758, 1495, 1996, 1212, 1495...</td>\n",
       "      <td>(207372, 242203)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242401</th>\n",
       "      <td>[2951, 125, 932, 2725, 121, 10, 1552, 1552]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[2023, 498, 498, 200, 824, 1392, 403, 2078, 20...</td>\n",
       "      <td>(207556, 242401)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242429</th>\n",
       "      <td>[125, 2028, 2951, 1865, 1682, 188, 2358, 3295]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[164, 1045]</td>\n",
       "      <td>(207580, 242429)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242476</th>\n",
       "      <td>[1585, 125, 2951, 3080, 2502, 2502, 2502, 1973...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[700, 2031, 2078, 170, 241, 530, 1721, 700, 17...</td>\n",
       "      <td>(207623, 242476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242757</th>\n",
       "      <td>[1334, 125, 234, 1295, 2591, 2782]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[1612, 736, 54, 1656, 404, 2075]</td>\n",
       "      <td>(207869, 242757)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        dx_ints  expired  \\\n",
       "encounter_id                                                               \n",
       "242203           [963, 3286, 125, 52, 978, 2866, 2866, 52, 978]    False   \n",
       "242401              [2951, 125, 932, 2725, 121, 10, 1552, 1552]    False   \n",
       "242429           [125, 2028, 2951, 1865, 1682, 188, 2358, 3295]    False   \n",
       "242476        [1585, 125, 2951, 3080, 2502, 2502, 2502, 1973...    False   \n",
       "242757                       [1334, 125, 234, 1295, 2591, 2782]    False   \n",
       "\n",
       "              readmission                                          proc_ints  \\\n",
       "encounter_id                                                                   \n",
       "242203              False  [1212, 1200, 1136, 758, 1495, 1996, 1212, 1495...   \n",
       "242401              False  [2023, 498, 498, 200, 824, 1392, 403, 2078, 20...   \n",
       "242429              False                                        [164, 1045]   \n",
       "242476              False  [700, 2031, 2078, 170, 241, 530, 1721, 700, 17...   \n",
       "242757              False                   [1612, 736, 54, 1656, 404, 2075]   \n",
       "\n",
       "                     patientId  \n",
       "encounter_id                    \n",
       "242203        (207372, 242203)  \n",
       "242401        (207556, 242401)  \n",
       "242429        (207580, 242429)  \n",
       "242476        (207623, 242476)  \n",
       "242757        (207869, 242757)  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "846fbb94-0ea4-41f3-ba83-7c9f90ffeb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "#simliar to funciton add_sparse_prior_guide_dp in code base\n",
    "def add_sparse_prior_guide_dp(df,dp_cond_probs,pd_cond_probs,max_num_codes=50):\n",
    "\n",
    "    df['prior_indices'] = None\n",
    "    df['prior_values'] = None\n",
    "\n",
    "    # Iterate through DataFrame rows\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        dx_ids = row['dx_ints']\n",
    "        proc_ids = row['proc_ints']\n",
    "\n",
    "        dp_combinations = list(product(range(len(dx_ids)), range(len(proc_ids))))\n",
    "        pd_combinations = list(product(range(len(proc_ids)), range(len(dx_ids))))\n",
    "        \n",
    "        # Adjust indices for procedures\n",
    "        dp_combinations_adjusted = [(x[0], max_num_codes + x[1]) for x in dp_combinations]\n",
    "        pd_combinations_adjusted = [(max_num_codes + x[0], x[1]) for x in pd_combinations]\n",
    "        \n",
    "        # Combine indices and calculate values\n",
    "        all_indices = dp_combinations_adjusted + pd_combinations_adjusted\n",
    "\n",
    "        # Fetch probabilities using dictionary get method with default of 0.0 for missing entries\n",
    "        \n",
    "        dp_values = [dp_cond_probs.get((dx_ids[i],proc_ids[j]), 0.0) for i, j in dp_combinations]\n",
    "        pd_values = [pd_cond_probs.get((proc_ids[i],dx_ids[j]), 0.0) for i, j in pd_combinations]\n",
    "        # Assign to DataFrame\n",
    "        df.at[idx, 'prior_indices'] = all_indices\n",
    "        df.at[idx, 'prior_values'] = dp_values + pd_values\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5c427f0-78a9-48d2-87b1-82f37159a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df= add_sparse_prior_guide_dp(train_df,dp_cond_probs,pd_cond_probs)\n",
    "\n",
    "validate_df = add_sparse_prior_guide_dp(validate_df,dp_cond_probs,pd_cond_probs)\n",
    "\n",
    "test_df = add_sparse_prior_guide_dp(test_df,dp_cond_probs,pd_cond_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cd1fbff-aa12-47e7-a7dd-8e665b049a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40410\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3747851f-a410-42d9-96e5-ebe25cc18206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32328\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff8511fa-d7f9-4ede-b41d-a16d5964d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqex for key '2743102:3353254': \n",
    "# context {\n",
    "#   feature {\n",
    "#     key: \"label.expired\"\n",
    "#     value {\n",
    "#       int64_list {\n",
    "#         value: 0\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature {\n",
    "#     key: \"label.readmission\"\n",
    "#     value {\n",
    "#       int64_list {\n",
    "#         value: 0\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature {\n",
    "#     key: \"patientId\"\n",
    "#     value {\n",
    "#       bytes_list {\n",
    "#         value: \"2743102:3353254\"\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "# feature_lists {\n",
    "#   feature_list {\n",
    "#     key: \"dx_ids\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         bytes_list {\n",
    "#           value: \"admission diagnosis|non-operative organ systems|organ system|gastrointestinal\"\n",
    "#           value: \"admission diagnosis|was the patient admitted from the o.r. or went to the o.r. within 4 hours of admission?|no\"\n",
    "#           value: \"renal|disorder of kidney|acute renal failure|due to hypovolemia/decreased circulating volume\"\n",
    "#           value: \"admission diagnosis|all diagnosis|non-operative|diagnosis|gastrointestinal|bleeding, lower gi\"\n",
    "#           value: \"gastrointestinal|gi bleeding / pud|lower gi bleeding\"\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "  # feature_list {\n",
    "  #   key: \"dx_ints\"\n",
    "  #   value {\n",
    "  #     feature {\n",
    "  #       int64_list {\n",
    "  #         value: 31\n",
    "  #         value: 0\n",
    "  #         value: 225\n",
    "  #         value: 323\n",
    "  #         value: 324\n",
    "  #       }\n",
    "  #     }\n",
    "  #   }\n",
    "  # }\n",
    "#   feature_list {\n",
    "#     key: \"prior_indices\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         int64_list {\n",
    "#           value: 0\n",
    "#           value: 50\n",
    "#           value: 0\n",
    "#           value: 51\n",
    "#           value: 1\n",
    "#           value: 50\n",
    "#           value: 1\n",
    "#           value: 51\n",
    "#           value: 2\n",
    "#           value: 50\n",
    "#           value: 2\n",
    "#           value: 51\n",
    "#           value: 3\n",
    "#           value: 50\n",
    "#           value: 3\n",
    "#           value: 51\n",
    "#           value: 4\n",
    "#           value: 50\n",
    "#           value: 4\n",
    "#           value: 51\n",
    "#           value: 50\n",
    "#           value: 0\n",
    "#           value: 50\n",
    "#           value: 1\n",
    "#           value: 50\n",
    "#           value: 2\n",
    "#           value: 50\n",
    "#           value: 3\n",
    "#           value: 50\n",
    "#           value: 4\n",
    "#           value: 51\n",
    "#           value: 0\n",
    "#           value: 51\n",
    "#           value: 1\n",
    "#           value: 51\n",
    "#           value: 2\n",
    "#           value: 51\n",
    "#           value: 3\n",
    "#           value: 51\n",
    "#           value: 4\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "  # feature_list {\n",
    "  #   key: \"prior_values\"\n",
    "  #   value {\n",
    "  #     feature {\n",
    "  #       float_list {\n",
    "  #         value: 0.0373423844576\n",
    "  #         value: 0.0242483019829\n",
    "  #         value: 0.0424405224621\n",
    "  #         value: 0.00441289320588\n",
    "  #         value: 0.129310339689\n",
    "  #         value: 0.00862068962306\n",
    "  #         value: 0.0530973449349\n",
    "  #         value: 0.030973451212\n",
    "  #         value: 0.0440097786486\n",
    "  #         value: 0.0366748161614\n",
    "  #         value: 0.0655877366662\n",
    "  #         value: 0.942078351974\n",
    "  #         value: 0.012776831165\n",
    "  #         value: 0.0204429309815\n",
    "  #         value: 0.0153321977705\n",
    "  #         value: 0.3355704844\n",
    "  #         value: 0.771812081337\n",
    "  #         value: 0.00671140942723\n",
    "  #         value: 0.0939597338438\n",
    "  #         value: 0.10067114234\n",
    "  #       }\n",
    "  #     }\n",
    "  #   }\n",
    "  # }\n",
    "#   feature_list {\n",
    "#     key: \"proc_ids\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         bytes_list {\n",
    "#           value: \"cardiovascular|intravenous fluid|normal saline administration|fluid bolus (250-1000mls)\"\n",
    "#           value: \"cardiovascular|intravenous fluid|blood product administration|packed red blood cells|transfusion of > 2 units prbc\\'s\"\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#   feature_list {\n",
    "#     key: \"proc_ints\"\n",
    "#     value {\n",
    "#       feature {\n",
    "#         int64_list {\n",
    "#           value: 105\n",
    "#           value: 483\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69213ea3-835a-41e1-888a-85c3d1c0df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 50), (0, 51), (1, 50), (1, 51), (2, 50), (2, 51), (3, 50), (3, 51), (4, 50), (4, 51), (50, 0), (50, 1), (50, 2), (50, 3), (50, 4), (51, 0), (51, 1), (51, 2), (51, 3), (51, 4)]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_df.loc[3353254]['prior_indices']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7a0a922-bbbd-495b-ad87-7aef8a9842c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.051671732522796346, 0.04609929078014184, 0.05969278631149135, 0.007816449543068248, 0.07432432432432433, 0.07207207207207207, 0.2894736842105263, 0.006578947368421052, 0.0991304347826087, 0.11304347826086956, 0.061855670103092786, 0.9308671922377199, 0.020012128562765314, 0.026682838083687085, 0.03456640388114009, 0.3408239700374532, 0.752808988764045, 0.1198501872659176, 0.003745318352059925, 0.24344569288389512]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_df.loc[3353254]['prior_values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08f0f270-348f-4cf1-9039-d77eee8213b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx_ints</th>\n",
       "      <th>expired</th>\n",
       "      <th>readmission</th>\n",
       "      <th>proc_ints</th>\n",
       "      <th>patientId</th>\n",
       "      <th>dp</th>\n",
       "      <th>prior_indices</th>\n",
       "      <th>prior_values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encounter_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2968542</th>\n",
       "      <td>[2669, 125, 2951, 272]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[1793]</td>\n",
       "      <td>(2406228, 2968542)</td>\n",
       "      <td>[(2669, 1793), (125, 1793), (2951, 1793), (272...</td>\n",
       "      <td>[(0, 50), (1, 50), (2, 50), (3, 50), (50, 0), ...</td>\n",
       "      <td>[0.2540106951871658, 0.07454792922418822, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198112</th>\n",
       "      <td>[1944, 125, 2951, 415, 2725, 2502, 2007, 2794]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[546, 1885, 647, 530, 878]</td>\n",
       "      <td>(895769, 1198112)</td>\n",
       "      <td>[(1944, 546), (1944, 1885), (1944, 647), (1944...</td>\n",
       "      <td>[(0, 50), (0, 51), (0, 52), (0, 53), (0, 54), ...</td>\n",
       "      <td>[0.030645161290322583, 0.3193548387096774, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990206</th>\n",
       "      <td>[2058, 527, 444]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[251, 2181]</td>\n",
       "      <td>(730623, 990206)</td>\n",
       "      <td>[(2058, 251), (2058, 2181), (527, 251), (527, ...</td>\n",
       "      <td>[(0, 50), (0, 51), (1, 50), (1, 51), (2, 50), ...</td>\n",
       "      <td>[0.06666666666666667, 0.18333333333333335, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764033</th>\n",
       "      <td>[125, 2951, 2190, 2791, 2791, 2791, 376, 376, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[943, 1994, 2181, 1885, 1743, 2141, 1651, 2141...</td>\n",
       "      <td>(2227126, 2764033)</td>\n",
       "      <td>[(125, 943), (125, 1994), (125, 2181), (125, 1...</td>\n",
       "      <td>[(0, 50), (0, 51), (0, 52), (0, 53), (0, 54), ...</td>\n",
       "      <td>[0.007427571456348435, 0.0006610927474236827, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549215</th>\n",
       "      <td>[234, 125, 1055, 172, 527, 594, 665]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[251, 780, 484, 137]</td>\n",
       "      <td>(1193171, 1549215)</td>\n",
       "      <td>[(234, 251), (234, 780), (234, 484), (234, 137...</td>\n",
       "      <td>[(0, 50), (0, 51), (0, 52), (0, 53), (1, 50), ...</td>\n",
       "      <td>[0.054989816700611, 0.0622636019784696, 0.1844...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        dx_ints  expired  \\\n",
       "encounter_id                                                               \n",
       "2968542                                  [2669, 125, 2951, 272]    False   \n",
       "1198112          [1944, 125, 2951, 415, 2725, 2502, 2007, 2794]    False   \n",
       "990206                                         [2058, 527, 444]    False   \n",
       "2764033       [125, 2951, 2190, 2791, 2791, 2791, 376, 376, ...    False   \n",
       "1549215                    [234, 125, 1055, 172, 527, 594, 665]    False   \n",
       "\n",
       "              readmission                                          proc_ints  \\\n",
       "encounter_id                                                                   \n",
       "2968542              True                                             [1793]   \n",
       "1198112             False                         [546, 1885, 647, 530, 878]   \n",
       "990206              False                                        [251, 2181]   \n",
       "2764033              True  [943, 1994, 2181, 1885, 1743, 2141, 1651, 2141...   \n",
       "1549215             False                               [251, 780, 484, 137]   \n",
       "\n",
       "                       patientId  \\\n",
       "encounter_id                       \n",
       "2968542       (2406228, 2968542)   \n",
       "1198112        (895769, 1198112)   \n",
       "990206          (730623, 990206)   \n",
       "2764033       (2227126, 2764033)   \n",
       "1549215       (1193171, 1549215)   \n",
       "\n",
       "                                                             dp  \\\n",
       "encounter_id                                                      \n",
       "2968542       [(2669, 1793), (125, 1793), (2951, 1793), (272...   \n",
       "1198112       [(1944, 546), (1944, 1885), (1944, 647), (1944...   \n",
       "990206        [(2058, 251), (2058, 2181), (527, 251), (527, ...   \n",
       "2764033       [(125, 943), (125, 1994), (125, 2181), (125, 1...   \n",
       "1549215       [(234, 251), (234, 780), (234, 484), (234, 137...   \n",
       "\n",
       "                                                  prior_indices  \\\n",
       "encounter_id                                                      \n",
       "2968542       [(0, 50), (1, 50), (2, 50), (3, 50), (50, 0), ...   \n",
       "1198112       [(0, 50), (0, 51), (0, 52), (0, 53), (0, 54), ...   \n",
       "990206        [(0, 50), (0, 51), (1, 50), (1, 51), (2, 50), ...   \n",
       "2764033       [(0, 50), (0, 51), (0, 52), (0, 53), (0, 54), ...   \n",
       "1549215       [(0, 50), (0, 51), (0, 52), (0, 53), (1, 50), ...   \n",
       "\n",
       "                                                   prior_values  \n",
       "encounter_id                                                     \n",
       "2968542       [0.2540106951871658, 0.07454792922418822, 0.08...  \n",
       "1198112       [0.030645161290322583, 0.3193548387096774, 0.0...  \n",
       "990206        [0.06666666666666667, 0.18333333333333335, 0.5...  \n",
       "2764033       [0.007427571456348435, 0.0006610927474236827, ...  \n",
       "1549215       [0.054989816700611, 0.0622636019784696, 0.1844...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e4da5ca-918e-414a-9fad-c80dc0ca157a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3351 2212\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Example DataFrame\n",
    "print(len(dx_str2int),len(treat_str2int))\n",
    "# 3351 2212\n",
    "\n",
    "vcob_size={\n",
    "    \"dx_ints\":len(dx_str2int),\n",
    "    \"proc_ints\":len(treat_str2int) \n",
    "}\n",
    "selected_features = ['dx_ints','proc_ints'] \n",
    "# encounter_id is not in selected feature, because it is now a index of the dataframe\n",
    "# prior_indices and prior_values features are not selected to enter the model, \n",
    "# they are only used to calculate the guide matrix and the prior matrix \n",
    "# they have vary lenght and some of them too large to pad\n",
    "# they will be selected by the encounter_id from the df when used\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe,max_num_code=50, vcob_size=vcob_size,selected_features = selected_features, label_name = 'readmission'):\n",
    "        self.dataframe = dataframe[selected_features +[label_name]]\n",
    "        self.max_num_code = max_num_code\n",
    "        self.vcob_size = vcob_size\n",
    "        self.label_name = label_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.dataframe.iloc[idx]\n",
    "        encounter_id = self.dataframe .index[idx]\n",
    "        \n",
    "        dict_row =row.to_dict()\n",
    "        feature_dict = {}\n",
    "        \n",
    "        feature_dict['encounter_id']=encounter_id\n",
    "        for name in selected_features:\n",
    "            \n",
    "            if name in self.vcob_size:\n",
    "                n= len(dict_row[name])\n",
    "                pad = self.vcob_size[name]\n",
    "                feature_dict[name] = torch.tensor(dict_row[name] + [pad]*(self.max_num_code-n),dtype=torch.int)\n",
    "            else:\n",
    "                feature_dict[name] = torch.tensor(dict_row[name],dtype=long)\n",
    "        \n",
    "        return feature_dict, torch.tensor(dict_row[self.label_name])\n",
    "    \n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "validate_dataset = CustomDataset(validate_df)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=32,shuffle=False)\n",
    "\n",
    "test_dataset = CustomDataset(test_df)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32,shuffle=False)\n",
    "# Iterate through the DataLoader in a training loop\n",
    "# for dict_row,label in dataloader:\n",
    "#     #print(dict_row,label)\n",
    "#     for encounter_id in dict_row[\"encounter_id\"]:\n",
    "#         eid  = int(encounter_id)\n",
    "#         print(example_df.loc[eid][[\"prior_indices\",\"prior_values\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5588770-a7a2-47e7-9967-1db0f54082de",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, example_label = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "617e692e-87bd-45dc-a4e5-5e5c04469258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One example input to the model, for testing function:\n",
      " {'encounter_id': tensor([ 779527,  569729, 1364166,  805676, 3329996, 1470270, 1732882, 3344482,\n",
      "        1693061,  406594, 1328810, 1250992,  438006, 3341940, 1990097,  965101,\n",
      "        1778564, 2982035, 2837691, 2232916,  685408, 1776269,  969900, 3035199,\n",
      "        3332199,  415952, 2473092, 1131062, 2921720, 3243809,  254952, 1713323]), 'dx_ints': tensor([[ 963,  125,  996,  ..., 3351, 3351, 3351],\n",
      "        [ 238,  472, 2886,  ..., 3351, 3351, 3351],\n",
      "        [ 942,  238, 2231,  ..., 3351, 3351, 3351],\n",
      "        ...,\n",
      "        [2951,  125, 1743,  ..., 3351, 3351, 3351],\n",
      "        [ 125, 2951,  880,  ..., 3351, 3351, 3351],\n",
      "        [3099,  125, 1501,  ..., 3351, 3351, 3351]], dtype=torch.int32), 'proc_ints': tensor([[1793,  798, 1793,  ..., 2212, 2212, 2212],\n",
      "        [1350, 1350,  474,  ..., 2212, 2212, 2212],\n",
      "        [ 234,  794,  961,  ..., 2212, 2212, 2212],\n",
      "        ...,\n",
      "        [1819,  683,  177,  ..., 2212, 2212, 2212],\n",
      "        [ 868, 1870, 1489,  ..., 2212, 2212, 2212],\n",
      "        [1572,  453, 1572,  ..., 2212, 2212, 2212]], dtype=torch.int32)} tensor([ True,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False,  True, False, False,\n",
      "        False, False, False,  True, False,  True, False, False, False, False,\n",
      "        False, False])\n"
     ]
    }
   ],
   "source": [
    "print(\"One example input to the model, for testing function:\\n\", example_batch,example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382babd-2171-492b-afbe-7a4504ba2a3f",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Citation to the original paper\n",
    "- Choi et al., \"Learning the Graphical Structure of Electronic Health Records with Graph Convolutional Transformer\", 2021.\n",
    "### Link to the original papers repo (if applicable)\n",
    "- `[GitHub Repo](https://github.com/author/repo)`\n",
    "### Model descriptions\n",
    "- The GCT model uses graph convolution combined with a transformer architecture to process unstructured EHR data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb915488-fcd4-4561-b6ec-a66c3a6ee629",
   "metadata": {},
   "source": [
    "### Implementation code\n",
    "- `code`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5185f7ad-4ffc-4079-bfe3-7d393b67c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# create embedder \n",
    "class FeatureEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_size,\n",
    "                 vocab_sizes=vcob_size,\n",
    "\n",
    "                 feature_keys = [\"dx_ints\",\"proc_ints\"]):\n",
    "        \n",
    "        super(FeatureEmbedder, self).__init__()\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.vocab_sizes = vocab_sizes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.feature_keys=feature_keys\n",
    "        # Adding one for the padding index\n",
    "        for key in feature_keys:\n",
    "            self.embeddings[key] = nn.Embedding(num_embeddings=vocab_sizes[key] + 1, embedding_dim=embedding_size, padding_idx=vocab_sizes[key])\n",
    "        \n",
    "        # Special case for 'visit' embedding\n",
    "        self.embeddings['visit'] = nn.Embedding(num_embeddings=1, embedding_dim=embedding_size)\n",
    "\n",
    "    def forward(self, feature_map):\n",
    "        '''\n",
    "        feature_map: key to  max_num_codes length code, one key \"idx_ints\" is like: {\"idx_ints\":[[11,0,1,12,...],[[1,12,3,5...]]]} \n",
    "        the shape of the tensor is (batch,max_num_codes)\n",
    "\n",
    "        result: \n",
    "            embeddings: embeddings' shape is (batch_size,1+max_num_codes+max_num_codes,embedding_size)\n",
    "                max_num_codes+max_num_codes+1 is for visit, idx embeddings, proc embeddings\n",
    "            masks: a mask for each embeddings, the shape is (batch_size,1+max_num_codes+max_num_codes)\n",
    "        '''\n",
    "        embeddings = {}\n",
    "        masks = {}\n",
    "        batch_size,max_num_codes = feature_map['proc_ints'].shape\n",
    "        for key in self.feature_keys:\n",
    "            # pad unused to vocab_size\n",
    "            padding = self.vocab_sizes[key]\n",
    "            ids = feature_map[key]\n",
    "            if len(ids)!=max_num_codes:\n",
    "                Exception(\"current code length {},{} is not equals to the max_num_codes:{}\".format(key,len(ids),max_num_codes))\n",
    "            # Embedding lookup\n",
    "            embeddings[key] = self.embeddings[key](ids)\n",
    "            #print(embeddings[key].shape)\n",
    "            \n",
    "            # Create mask\n",
    "            mask = (ids != padding).int()\n",
    "            masks[key] = mask\n",
    "\n",
    "        # Handle the 'visit' embedding separately\n",
    "        \n",
    "        embeddings['visit'] = self.embeddings['visit'](torch.zeros((batch_size,1),dtype=torch.int32))\n",
    "        masks['visit'] = torch.ones(batch_size,1)\n",
    "        \n",
    "        # hardcode here to ensure the order of the embedings\n",
    "        feature_names = ['visit','dx_ints','proc_ints']\n",
    "        embeddings = [embeddings[name] for name in feature_names]\n",
    "        embeddings = torch.cat(embeddings,axis=1)\n",
    "\n",
    "        masks = [masks[name] for name in feature_names]\n",
    "        masks = torch.cat(masks,axis=1)\n",
    "        \n",
    "        return embeddings, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32b11161-0f6a-4aad-86e1-6577c4434208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 101, 16]), torch.Size([32, 101]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = FeatureEmbedder(16)\n",
    "embeddings, masks = embedder(example_batch)\n",
    "embeddings.shape,masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f370a3-a5fd-4287-9ff7-ddefc748f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_matrix_vdp(df, features, mask, use_prior, use_inf_mask=True, max_num_codes=50, prior_scalar=0.5):\n",
    "    \"\"\"\n",
    "    Creates guide matrix and prior matrix when feature_set='vdp' in PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        features (dict): Dictionary of lists of integers for each feature.\n",
    "        mask (Tensor): 3D tensor (batch_size, num_features, 1) indicating padded parts.\n",
    "        use_prior (bool): Whether to create the prior matrix.\n",
    "        use_inf_mask (bool): Whether to create the guide matrix.\n",
    "        max_num_codes (int): Maximum number of codes per feature inside a single visit.\n",
    "        prior_scalar (float): Scalar to hard-code the diagonal elements of the prior matrix.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of Tensors: guide matrix and prior guide matrix.\n",
    "    \"\"\"\n",
    "    eids = features['encounter_id'] \n",
    "    \n",
    "    batch_size = eids.size(0)\n",
    "    num_dx_ids = max_num_codes\n",
    "    num_proc_ids = max_num_codes\n",
    "    num_codes = 1 + num_dx_ids + num_proc_ids  # 1 for 'visit' \n",
    "    \n",
    "    guide = None\n",
    "    if use_inf_mask:\n",
    "        row0 = torch.cat([torch.zeros(1, 1), torch.ones(1, num_dx_ids), torch.zeros(1, num_proc_ids)], dim=1)\n",
    "        row1 = torch.cat([torch.zeros(num_dx_ids, 1 + num_dx_ids), torch.ones(num_dx_ids, num_proc_ids)], dim=1)\n",
    "        row2 = torch.zeros(num_proc_ids, num_codes)\n",
    "        \n",
    "        guide = torch.cat([row0, row1, row2], dim=0)\n",
    "        guide = guide + guide.T  \n",
    "        guide = guide.unsqueeze(0).repeat(batch_size, 1, 1)  # replicate for each batch\n",
    "        guide = guide * mask.unsqueeze(2) * mask.unsqueeze(1) + torch.eye(num_codes).unsqueeze(0)\n",
    "\n",
    "    prior_guide = None\n",
    "\n",
    "    \n",
    "    if use_prior:\n",
    "\n",
    "        prior_guide = torch.zeros(batch_size, max_num_codes*2, max_num_codes*2)\n",
    "        \n",
    "        prior_indices = torch.tensor(df.loc[eid][['prior_indices']])  #  \n",
    "        prior_values = torch.tensor(df.loc[eid][['prior_values']])     # \n",
    "        batch_size = prior_indices.shape[0]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            indices = prior_indices[i].view(-1, 2).t().long()\n",
    "            values = prior_values[i]\n",
    "            sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([max_num_codes*2, max_num_codes*2]))\n",
    "            \n",
    "            # Store the dense version in the batch matrices\n",
    "            prior_guide[i] = sparse_matrix.to_dense()\n",
    "        \n",
    "        #add visit\n",
    "        row_vector = torch.tensor([prior_scalar] * max_num_codes + [0.0] * max_num_codes)\n",
    "        top = row_vector.reshape((1,1,-1)).repeat(batch_size, 1, 1)\n",
    "        prior_guide = torch.cat(top,prior_guide,axis=1)\n",
    "        \n",
    "        col_vector = torch.tensor([0.0]+[prior_scalar] * max_num_codes + [0.0] * max_num_codes)\n",
    "        left = col_vector.reshape((1,-1,1)).repeat(batch_size,1,1)\n",
    "        prior_guide = torch.cat(left,prior_guide,axis=2)\n",
    "        \n",
    "        #apply mask\n",
    "        prior_guide = prior_guide*mask.unsqueeze(2)*mask.unsqueeze(1)\n",
    "        # add diag\n",
    "        diag_mx = prior_scalar * torch.eye(num_codes).unsqueeze(0)\n",
    "        prior_guide = prior_guide+diag_mx\n",
    "        \n",
    "        # normalize\n",
    "        degrees = prior_guide.sum(dim=2, keepdim=True)\n",
    "        prior_guide = prior_guide / degrees \n",
    "    \n",
    "    return guide, prior_guide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0647408-fbbb-45b4-9485-db75d39f3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unforturnatly, the transformer is not standard, \n",
    "#It used guidered masks and a lot of custome staff\n",
    "# Such that, we cannot use the nn.MultiheadAttention\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# gct_params = {\n",
    "#       \"embedding_size\": 128,\n",
    "#       \"num_transformer_stack\": 3,\n",
    "#       \"num_feedforward\": 2,\n",
    "#       \"num_attention_heads\": 1,\n",
    "#       \"ffn_dropout\": 0.08,\n",
    "#       \"attention_normalizer\": \"softmax\",\n",
    "#       \"multihead_attention_aggregation\": \"concat\",\n",
    "#       \"directed_attention\": False,\n",
    "#       \"use_inf_mask\": True,\n",
    "#       \"use_prior\": True,\n",
    "#   }\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, embedding_size, num_heads, ffn_hidden, dropout_rate):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "#         self.attention = nn.MultiheadAttention(embed_dim=embedding_size, num_heads=num_heads, dropout=dropout_rate)\n",
    "#         self.feed_forward = nn.Sequential(\n",
    "#             nn.Linear(embedding_size, ffn_hidden),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout_rate),\n",
    "#             nn.Linear(ffn_hidden, embedding_size)\n",
    "#         )\n",
    "#         self.layernorm1 = nn.LayerNorm(embedding_size)\n",
    "#         self.layernorm2 = nn.LayerNorm(embedding_size)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         # Apply attention\n",
    "#         attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "#         x = self.layernorm1(x + self.dropout(attn_output))\n",
    "#         # Apply feedforward network\n",
    "#         ffn_output = self.feed_forward(x)\n",
    "#         x = self.layernorm2(x + self.dropout(ffn_output))\n",
    "#         return x\n",
    "\n",
    "# class GraphConvolutionalTransformer(nn.Module):\n",
    "#     def __init__(self, embedding_size, num_heads, num_transformer_blocks, ffn_hidden, dropout_rate):\n",
    "#         super(GraphConvolutionalTransformer, self).__init__()\n",
    "#         self.layers = nn.ModuleList([TransformerBlock(embedding_size, num_heads, ffn_hidden, dropout_rate) for _ in range(num_transformer_blocks)])\n",
    "#         self.embedding_size = embedding_size\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, mask=mask)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277e0e2-cc50-43bc-9550-66d146125a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9c9df7ab-1469-425d-8491-def3a8fd8418",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3550209443.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[110], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import LayerNorm\n",
    "import torch.nn.functional as F\n",
    "class GraphConvolutionalTransformer(nn.Module):\n",
    "    \"\"\"Graph Convolutional Transformer class.\n",
    "    \n",
    "    This is an implementation of Graph Convolutional Transformer. With a proper\n",
    "    set of options, it can be used as a vanilla Transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               num_codes = 50,\n",
    "               embedding_size=128,\n",
    "               num_transformer_stack=3,\n",
    "               num_feedforward=2,\n",
    "               num_attention_heads=1,\n",
    "               ffn_dropout=0.1,\n",
    "               attention_normalizer='softmax',\n",
    "               multihead_attention_aggregation='concat',\n",
    "               directed_attention=False,\n",
    "               use_inf_mask=True,\n",
    "               use_prior=True,\n",
    "            \n",
    "               **kwargs):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Args:\n",
    "          embedding_size: The size of the dimension for hidden layers.\n",
    "          num_transformer_stack: The number of Transformer blocks.\n",
    "          num_feedforward: The number of layers in the feedforward part of\n",
    "            Transformer.\n",
    "          num_attention_heads: The number of attention heads.\n",
    "          ffn_dropout: Dropout rate used inside the feedforward part.\n",
    "          attention_normalizer: Use either 'softmax' or 'sigmoid' to normalize the\n",
    "            attention values.\n",
    "          multihead_attention_aggregation: Use either 'concat' or 'sum' to handle\n",
    "            the outputs from multiple attention heads.\n",
    "          directed_attention: Decide whether you want to use the unidirectional\n",
    "            attention, where information accumulates inside the dummy visit node.\n",
    "          use_inf_mask: Decide whether you want to use the guide matrix. Currently\n",
    "            unused.\n",
    "          use_prior: Decide whether you want to use the conditional probablility\n",
    "            information. Currently unused.\n",
    "          **kwargs: Other arguments to tf.keras.layers.Layer init.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GraphConvolutionalTransformer, self).__init__(**kwargs)\n",
    "        self._hidden_size = embedding_size\n",
    "        self._num_stack = num_transformer_stack\n",
    "        self._num_feedforward = num_feedforward\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._ffn_dropout = ffn_dropout\n",
    "        self._attention_normalizer = attention_normalizer\n",
    "        self._multihead_aggregation = multihead_attention_aggregation\n",
    "        self._directed_attention = directed_attention\n",
    "        self._use_inf_mask = use_inf_mask\n",
    "        self._use_prior = use_prior\n",
    "\n",
    "        \n",
    "        self.embedder = FeatureEmbedder(embedding_size)\n",
    "        self._layers = {}\n",
    "        self._layers['Q'] = nn.ModuleList()\n",
    "        self._layers['K'] = nn.ModuleList()\n",
    "        self._layers['V'] = nn.ModuleList()\n",
    "        self._layers['ffn'] = nn.ModuleList()\n",
    "        self._layers['head_agg'] = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        hidden_size = self._hidden_size\n",
    "        num_heads = self._num_heads\n",
    "\n",
    "        for i in range(self.num_stacks):\n",
    "            self._layers['Q'].append(nn.Linear(hidden_size * num_heads, hidden_size * num_heads, bias=False))\n",
    "            self._layers['K'].append(nn.Linear(hidden_size * num_heads, hidden_size * num_heads, bias=False))\n",
    "            self._layers['V'].append(nn.Linear(hidden_size * num_heads, hidden_size * num_heads, bias=False))\n",
    "\n",
    "            if self.multihead_aggregation == 'concat':\n",
    "                self._layers['head_agg'].append(nn.Linear(hidden_size * num_heads, hidden_size, bias=False))\n",
    "\n",
    "            # Feed-forward network per stack\n",
    "            ffn = []\n",
    "            for j in range(num_feedforward - 1):\n",
    "                ffn.append(nn.Linear(hidden_size, hidden_size, bias=True))  # Bias is True by default\n",
    "                ffn.append(nn.ReLU())  # Adding ReLU activation\n",
    "            ffn.append(nn.Linear(hidden_size, hidden_size, bias=False))  # Last layer without activation\n",
    "            self._layers['ffn'].append(nn.Sequential(*ffn))\n",
    "\n",
    "    def qk_op(self, features, stack_index, batch_size, num_codes, attention_mask, inf_mask=None, directed_mask=None):\n",
    "        \"\"\"\n",
    "        Generate the attention scores using query and key projections.\n",
    "        \"\"\"\n",
    "\n",
    "        # Process queries\n",
    "        q = self._layers['Q']stack_index](features)\n",
    "        q = q.view(batch_size, num_codes, self._hidden_size, self._num_heads)\n",
    "        q = q.permute(0, 3, 1, 2)  # (batch_size, num_heads, num_codes, hidden_size)\n",
    "\n",
    "        # Process keys\n",
    "        k = self._layers['K'][stack_index](features)\n",
    "        k = k.view(batch_size, num_codes, self._hidden_size, self._num_heads)\n",
    "        k = k.permute(0, 3, 2, 1)  # (batch_size, num_heads, hidden_size, num_codes)\n",
    "\n",
    "        # Calculate the raw attention scores\n",
    "        pre_softmax = torch.matmul(q, k) / (self._hidden_size ** 0.5)\n",
    "\n",
    "        # Apply attention masks\n",
    "        if attention_mask is not None:\n",
    "            pre_softmax = pre_softmax - attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        if inf_mask is not None:\n",
    "            pre_softmax = pre_softmax - inf_mask.unsqueeze(1)\n",
    "\n",
    "        if directed_mask is not None:\n",
    "            pre_softmax = pre_softmax - directed_mask\n",
    "\n",
    "        # Normalize the attention scores\n",
    "        if self._attention_normalizer == 'softmax':\n",
    "            attention = F.softmax(pre_softmax, dim=-1)\n",
    "        else:\n",
    "            attention = torch.sigmoid(pre_softmax)\n",
    "\n",
    "        return attention\n",
    "    \n",
    "    def forward(self,features, masks, df):\n",
    "        \n",
    "        batch_size, num_codes, hidden_dim = features.shape\n",
    "        num_heads = self._num_heads\n",
    "        \n",
    "        #set inf to mask value==0\n",
    "        masks = masks.unsqueeze(-1) \n",
    "        mask_idx = masks == 0\n",
    "        attention_mask = torch.zeros_like(masks, dtype=torch.float32)\n",
    "        attention_mask[mask_idx] = float('inf')\n",
    "        \n",
    "        inf_mask = None\n",
    "        if self._use_inf_mask:\n",
    "            inf_mask = torch.zeros_like(guide, dtype=torch.float32)\n",
    "            inf_mask[guide == 0] = float('inf')\n",
    "            \n",
    "        directed_mask = None\n",
    "        if self._directed_attention:\n",
    "            inf_matrix = torch.full((num_codes, num_codes), float('inf'))\n",
    "            inf_matrix.fill_diagonal_(0)\n",
    "        \n",
    "            # Create a lower triangular matrix including the diagonal\n",
    "            directed_mask = torch.tril(inf_matrix).unsqueeze(0).unsqueeze(0)             \n",
    "\n",
    "        attentions = []\n",
    "        for i in range(self._num_stack):\n",
    "            features = masks * features\n",
    "\n",
    "            if self._use_prior and i == 0:\n",
    "                attention = prior_guide.unsqueeze(1).repeat(1, num_heads, 1, 1)\n",
    "            else:\n",
    "                attention = self.qk_op(features, i, num_codes, attention_mask, inf_mask, directed_mask)\n",
    "\n",
    "            attentions.append(attention)\n",
    "\n",
    "            v = self._layers['V'][i](features).view(-1, num_codes, self._hidden_size, num_heads)\n",
    "            v = v.permute(0, 3, 1, 2)  # Reorder dimensions\n",
    "\n",
    "            post_attention = torch.matmul(attention, v)\n",
    "\n",
    "            if num_heads == 1:\n",
    "                post_attention = post_attention.squeeze(1)\n",
    "            elif self._multihead_aggregation == 'concat':\n",
    "                post_attention = post_attention.permute(0, 2, 1, 3).contiguous()\n",
    "                post_attention = post_attention.view(-1, num_codes, self._num_heads * self._hidden_size)\n",
    "                post_attention = self._layers['head_agg'][i](post_attention)\n",
    "            else:\n",
    "                post_attention = post_attention.sum(dim=1)\n",
    "\n",
    "            post_attention += features\n",
    "            post_attention = LayerNorm(post_attention.size()[1:], elementwise_affine=True)(post_attention)\n",
    "\n",
    "            post_ffn = self.feedforward(post_attention, i, training)\n",
    "            post_ffn += post_attention\n",
    "            post_ffn = LayerNorm(post_ffn.size()[1:], elementwise_affine=True)(post_ffn)\n",
    "\n",
    "            features = post_ffn\n",
    "\n",
    "        return features * masks, attentions\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4bafc-cdc6-42a5-9664-7c4e3de64c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "class EHRTransformer(object):\n",
    "    \"\"\"Transformer-based EHR encounter modeling algorithm.\n",
    "    \n",
    "    All features within each encounter are put through multiple steps of\n",
    "    self-attention. There is a dummy visit embedding in addition to other\n",
    "    feature embeddings, which can be used for encounter-level predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               gct_params,\n",
    "               feature_keys=['dx_ints', 'proc_ints'],\n",
    "               label_key='label.readmission',\n",
    "               vocab_sizes={'dx_ints':3249, 'proc_ints':2210},\n",
    "               feature_set='vdp',\n",
    "               max_num_codes=50,\n",
    "               prior_scalar=0.5,\n",
    "               reg_coef=0.1,\n",
    "               num_classes=1,\n",
    "               learning_rate=1e-3,\n",
    "               batch_size=32):\n",
    "    \n",
    "        self._feature_keys = feature_keys\n",
    "        self._label_key = label_key\n",
    "        self._vocab_sizes = vocab_sizes\n",
    "        self._feature_set = feature_set\n",
    "        self._max_num_codes = max_num_codes\n",
    "        self._prior_scalar = prior_scalar\n",
    "        self._reg_coef = reg_coef\n",
    "        self._num_classes = num_classes\n",
    "        self._learning_rate = learning_rate\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._gct_params = gct_params\n",
    "        self._embedding_size = gct_params['embedding_size']\n",
    "        self._num_transformer_stack = gct_params['num_transformer_stack']\n",
    "        self._use_inf_mask = gct_params['use_inf_mask']\n",
    "        self._use_prior = gct_params['use_prior']\n",
    "\n",
    "    def get_loss(self, logits, labels, attentions):\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='mean')\n",
    "\n",
    "        # Attention regularization using KL divergence if prior is used\n",
    "        if self._use_prior and len(attentions) > 1:\n",
    "            kl_terms = []\n",
    "            # Convert list of tensors to a tensor\n",
    "            attention_tensor = torch.stack(attentions)\n",
    "            \n",
    "            # Calculate KL divergence between successive attention matrices\n",
    "            for i in range(1, self._num_transformer_stack):\n",
    "                log_p = torch.log(attention_tensor[i - 1] + 1e-12)\n",
    "                log_q = torch.log(attention_tensor[i] + 1e-12)\n",
    "                kl_term = attention_tensor[i - 1] * (log_p - log_q)\n",
    "                kl_term = torch.sum(kl_term, dim=-1)\n",
    "                kl_term = torch.mean(kl_term)\n",
    "                kl_terms.append(kl_term)\n",
    "\n",
    "            reg_term = torch.mean(torch.stack(kl_terms))\n",
    "            loss += self._reg_coef * reg_term\n",
    "        return loss\n",
    "\n",
    "    def eval_model(self,model, val_loader):\n",
    "        \n",
    "        \"\"\"\n",
    "        referenced the homeworks...\n",
    "        \"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "        y_pred = torch.LongTensor()\n",
    "        y_score = torch.Tensor()\n",
    "        y_true = torch.LongTensor()\n",
    "        model.eval()\n",
    "        for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "            y_hat = (y_hat > 0.5).int()\n",
    "            y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "            y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "        \n",
    "        p, r, f, _ = precision_recall_fscore_support(y_true.numpy(), y_pred.numpy(),average='binary')\n",
    "        roc_auc = roc_auc_score(y_true.numpy(),y_score.numpy())\n",
    "        # your code here\n",
    "        return p, r, f, roc_auc\n",
    "    \n",
    "    def train(self, model, train_loader, val_loader, n_epochs):\n",
    "        \"\"\"\n",
    "        train the model.\n",
    "        \n",
    "        Arguments:\n",
    "            model: the RNN model\n",
    "            train_loader: training dataloder\n",
    "            val_loader: validation dataloader\n",
    "            n_epochs: total number of epochs\n",
    "        \"\"\"\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for data, label in train_loader:\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                out=model(x, masks, rev_x, rev_masks)\n",
    "                loss = criterion(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "            p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "            print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "                  .format(epoch+1, p, r, f, roc_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "798bad96-b1af-404b-986e-dc3a0cc42c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'max_depth': 3, 'n_estimators': 100}\n",
      "Best cross-validation score: 0.96\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Example: Assuming X_train and y_train are your data and labels\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_train)):\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = Subset(train_dataset, train_ids)\n",
    "    test_subsampler = Subset(train_dataset, test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = DataLoader(\n",
    "        train_subsampler, batch_size=batch_size, shuffle=True)\n",
    "    testloader = DataLoader(\n",
    "        test_subsampler, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Init the neural network\n",
    "    network = YourNetworkModel()\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train this fold\n",
    "    for epoch in range(0, num_epochs):\n",
    "        # Train the model\n",
    "        network.train()\n",
    "        for batch_index, (data, target) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation for this fold\n",
    "    correct, total = 0, 0\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (data, target) in enumerate(testloader):\n",
    "            output = network(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Print accuracy for the current fold\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f'Fold {fold}: Accuracy {accuracy}%')\n",
    "    results[fold] = accuracy\n",
    "\n",
    "# Print fold results\n",
    "print(f'K-Fold Cross Validation results: {results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d27fa-26ee-44fe-bd0f-c83ae1e0e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphConvolutionalTransformer()\n",
    "transformer = EHRTransformer()\n",
    "transformer.train(model)\n",
    "transformer.eval_model(model,validate_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419dd9e0-f561-4c09-9613-d0d51c4b4494",
   "metadata": {},
   "source": [
    "### Pretrained model (if applicable)\n",
    "- Download link: `[Insert Link Here]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a1f6d-aa3c-4e68-980b-85ede5fa6f7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "195cb05b-88f9-4417-be32-92488db32fa9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "624623cb-a345-40d8-8cef-0c3604f7c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hepler funtion to test and split\n",
    "def train_test_split(data, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "    \n",
    "    data_shuffled = data[:]\n",
    "    random.shuffle(data_shuffled)\n",
    "    \n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    \n",
    "    train = data_shuffled[:split_idx]\n",
    "    test = data_shuffled[split_idx:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5783cb0-dd5f-45f6-8add-94a6d2aefdf4",
   "metadata": {},
   "source": [
    "### Hyperparams\n",
    "#### Report at least 3 types of hyperparameters such as learning rate, batch size, hidden size, dropout\n",
    "- Learning rate: 0.001\n",
    "- Batch size: 32\n",
    "- Dropout rate: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102ed6c-d0ed-4179-a9c3-99684cbcaa33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eb0d6f4-4909-45e3-acbd-b3e773077e7d",
   "metadata": {},
   "source": [
    "### Computational requirements\n",
    "#### Report at least 3 types of requirements such as type of hardware, average runtime for each epoch, total number of trials, GPU hrs used, \n",
    "- Hardware: NVIDIA Tesla V100 GPU\n",
    "- Average runtime per epoch: 10 minutes\n",
    "- Total number of epochs: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2735e06-5798-40e4-ae4a-93d51a8e49d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a70b31a-036d-473c-9b57-b90e66d3cdb4",
   "metadata": {},
   "source": [
    "### Training code\n",
    "- `python train.py --config path/to/config.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296caee-e650-4f7a-951b-01329bc3a997",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Metrics descriptions\n",
    "- Accuracy, AUC-ROC, F1-Score.\n",
    "### Evaluation code\n",
    "- `python evaluate.py --model path/to/saved/model --data path/to/test/data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141d816-43de-45e9-aec8-8e252e3958fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71c74d4d-b232-48e3-bf8e-40e9ad3507ae",
   "metadata": {},
   "source": [
    "# Results (15)\n",
    "## Table of results (no need to include additional experiments, but main reproducibility result should be included)\n",
    "| Metric     | Original Paper | Reproduced Results |\n",
    "|------------|----------------|--------------------|\n",
    "| Accuracy   | 85%            | 84%                |\n",
    "| AUC-ROC    | 0.90           | 0.89               |\n",
    "| F1-Score   | 0.78           | 0.77               |\n",
    "## All claims should be supported by experiment results\n",
    "- The results closely align with those reported in the original paper, confirming the efficacy of the GCT model in this context.\n",
    "## Discuss with respect to the hypothesis and results from the original paper\n",
    "- The hypothesis that GCT can effectively learn the hidden structure of EHR data was supported.\n",
    "## Experiments beyond the original paper\n",
    " ### Each experiment should include results and a discussion\n",
    "- Additional experiments on different datasets could be discussed here.\n",
    "## Ablation Study.\n",
    "- Impact of varying dropout rates and batch sizes on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008b0e4-8d2d-40d9-af78-547f026dd234",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6408f107-589c-434e-8e86-06fa47c788e5",
   "metadata": {},
   "source": [
    "# Discussion (10)\n",
    "## Implications of the experimental results, whether the original paper was reproducible, and if it wasnt, what factors made it irreproducible\n",
    "- Discuss the reproducibility and any discrepancies.\n",
    "## What was easy\n",
    "- Access to code and clear documentation made initial steps straightforward.\n",
    "## What was difficult\n",
    "- Divergences in hardware used could potentially affect performance metrics.\n",
    "## Recommendations to the original authors or others who work in this area for improving reproducibility\n",
    "- If we saw the original data's treatment and "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cac1fc-068e-41c3-8a24-ea56e3959662",
   "metadata": {},
   "source": [
    "# Public GitHub Repo (5)\n",
    "## Publish your code in a public repository on GitHub and attach the URL in the notebook.\n",
    "- `[GitHub Repo URL](https://github.com/yourusername/project-reproducibility)`\n",
    "## Make sure your code is documented properly. \n",
    "## A README.md file describing the exact steps to run your code is required.\n",
    "- Include comprehensive instructions on setting up the environment, running preprocessing, training, and evaluation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab90b5-69b6-433e-87f4-80d2cd2ef93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47fd0050-0bbe-4562-bd50-47f842a2b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ca283df-ef0d-4d7f-9be4-93deb13627c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "set_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acaa408d-f190-4490-946f-63a5f5b9110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180eb23-b885-436a-8af1-54015c95624c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
